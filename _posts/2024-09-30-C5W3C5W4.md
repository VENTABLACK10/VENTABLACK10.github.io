---
layout: single
title:  "Sequence Models & Attention Mechanism & Transformer Network"
---
## 3. Sequence Models & Attention Mechanism

### 3.1 Basic Models
* Seq2Seq Model: input sequence를 encoder network를 통해 처리하여 encoding된 vector를 decoder network로 전달하여 output sequence를 생성하는 신경망을 훈련시킨 모델.                
![photo 140](/assets/img/blog/img140.png)              
* encoder network: input sequence를 받아 벡터로 encoding한다. ex) RNN, GRU, LSTM 등
* decoder netwrok: encoding된 vector를 기반으로 한 단어씩 output sequence를 생성한다. ex) 영어 번역
* 이미지에서 추출한 feature vector를 Seq2Seq Model의 input으로 사용하면, 이미지에 대한 설명을 생성하는 image captioning에 활용할 수 있다.                
![photo 141](/assets/img/blog/img141.png)             
* 번역과 image captioning 모두 Seq2Seq 구조를 사용하지만, 번역에서는 무작위 출력보다는 가장 가능성이 높은 번역을 선택해야 하는 경우가 많다.

### 3.2 Picking the Most likely Sentence
* machine translation model은 conditional language model로, 입력된 문장을 조건으로 하여 출력될 문장의 확률을 모델링하는 방식이다.
* language model은 주어진 문장의 확률을 추정하여 문장을 생성하는데 사용되지만, machine translation에서는 입력된 문장에 대한 대응 번역을 조건부 확률로 계산하여 가장 적합한 번역을 찾아낸다.               
![photo 142](/assets/img/blog/img142.png)               
* machine translation model은 input sequence를 encoder로 처리한 encoding vector를 decoder network로 전달하여 output sequence를 생성한다. 이는 language mode과 유사하지만, 항상 zero vector로 시작하는 대신 입력 문장의 encoding된 표현을 사용한다.
* conditional language model에서는 가장 확률이 높은 출력을 찾는 것이 중요하므로 한 번에 하나의 가장 높은 확률의 단어를 선택하는 greedy search 방식 보다는 beam search algorithm을 활용해 조건부 확률을 극대화 하는 translation을 찾는다.              
![photo 143](/assets/img/blog/img143.png)         

### 3.3 Beam Search
* Beam Search algorithm: 각 단계에서 여러 가능성을 동시에 고려하여 가장 확률이 높은 단어들을 추적해 나가는 탐색 방법이다.           
![photo 144](/assets/img/blog/img144.png)                 
* Bean Search algorithm 작동 방식
  1. 입력 문장에 대해 각 단어의 확률을 계산하고 가장 확률이 높은 상위 B개의 단어를 선택한다. B는 빔 너비(폭)으로 동시에 고려하는 후보 문장의 수를 결정하는 값이다.
  2. 첫 번째 단어로 선택된 각 단어에 대해 두 번째 단어의 확률을 계산한다. 이후 두 단어의 조합 중에서 상위 B개의 쌍을 선택한다.
  3. 선택된 단어 쌍을 기반으로 세 번째 단어의 확률을 계산하고 상위 B개의 조합을 계속 추적한다. 이와 같은 방식으로 문장을 확장해 나아가면서 가능성이 높은 조합을 찾아간다.
  4. 문장의 끝(EOS) 기호가 나올 대까지 1~3을 반복한다. 아후 가장 성능이 좋은 번역을 출력한다.

### 3.4 Refinements to Beam Search
* Beam Search algorithm 단점: 문장이 길어질수록 더 많은 단어를 포함하므로 여러 확률을 곱하게 되어 최종 확률값이 매우 작아지는 문제가 발생한다.
* 해당 문제를 보완하기 위해 Length normalization 사용한다.
* Beam Search에서의 확률은 여러 단어의 조건부 확률을 곱한 값이므로 수치적으로 매우 작은 값을 다룬다. 이를 해결하기 위해 로그 확률을 취해 안정적인 계산 결과를 얻을 수 있다.             
![photo 146](/assets/img/blog/img146.png)            
* Length normalization: 로그 확률을 문장의 길이로 나누어 정규화 하는 방법             
![photo 145](/assets/img/blog/img145.png)      
* beam 너비(폭)을 의미하는 B값이 커질수록 더 많은 가능성을 고려하게 되어 성능이 좋아질 수 있지만, 계산 비용이 증가하고 메모리 소모가 커진다. 또한, 어느 정도 이상으로 증가시키면 성능도 크게 증가하지 않는다.
* 일반적으로 B값은 3에서 10정도로 적절한 값을 설정한다.

### 3.5 Error Analysis in Beam Search
* Error Analysis in Beam Search : beam search는 여러 가능성 중 상위 B개의 번역을 고려하지만 때때로 잘못 번역을 선택할 수 있다.
* beam search의 결과가 사람 번역(y^*)보다 낮은 품질의 번역(y-hat)을 생성했을 때, 두 가지 오류 가능성을 고려해야 한다.
  1. beam search 자체의 문제
  2. RNN model의 성능 문제
* Error Analysis 과정
  1. P(y^*|X)와 P(y-hat|x)를 계산하여 사람의 번역과 모델이 선택한 번역의 확률을 비교한다.
  2. beam search error = P(y*) > P(y-hat)
  3. RNN model error = P(y*) <= P(y-hat)
* Error Analysis을 통한 개선
  1. beam search error: B를 늘리거나 검색 알고리즘을 개선한다.
  2. RNN model error: model architecture를 개선하거나 더 많은 데이터로 학습시킨다.

### 3.6 Attention Model Intuition
* 기존 encoder-decoder 구조는 전체 문장을 한 번에 encoding하고 translation을 생성하는 방식이기 때문에 긴 문장에서는 성능이 떨어진다.     
![photo 147](/assets/img/blog/img147.png)          
* Attention Model: encoder의 입력 중에서 decoder가 특정 출력 단어를 생성할 때, 중요한 정보에 집중할 수 있도록 가중치를 부여하는 모델로 긴 문장을 처리할 때도 성능 저하를 방지한다.
* encoder-decoder architecture: 입력 문장을 encoder로 처리하여 각 단어를 생성하고 decoder를 통해 문장의 특정 부분에 집중해 번역을 생성한다.
* attention weight: 각 단어를 번역할 때의 가중치
* context vector: attention weight를 바탕으로 문장의 특정 부분에 집중하여 RNN translation을 생성한다.

### 3.7 Attention Model
* Attention Model은 출력 단어 생성 시 각 입력 단어에 가중치를 이용해 집중 정도를 다르게 설정한다.        
![photo 148](/assets/img/blog/img148.png)        
* Attention Model 작동 방식
  1. BRNN, GRU, LSTM을 사용하여 입력 문장의 각 단어에 대해 순방향 및 역방향 RNN을 사용해 각 단어에 대한 feature vector를 생성한다. 해당 feature vector는 각 단어의 의미와 주변 context 잘 표현한다.
  2. 출력 번역의 각 단어를 생성하는 과정에서, Decoder는 이전 단계의 은닉 상태와 입력 문장의 각 단어에 대한 feature vector를 기반으로 attention weight를 계산한다.
  3. 계산된 attention weight(α)는 softmax 함수로 정규화되어 가중치 합이 1이 되도록 한다.
  4. 최종적으로 attention weight(α)을 사용하여 입력 문장의 각 단어에 대한 집중도를 반영한 context vector를 생성한다. 해당 context vector를 decoder에 전달하여 다음 출력 단어를 생성하는 데 사용한다.
* attention weight(α)는 소규모 신경망을 통해 계산되고 경사 하강법을 사용해 훈련한다.
* attention weight(α) example                         
![photo 149](/assets/img/blog/img149.png)                 

## 4. Transformer Network

### 4.1 Self-Attention
* Self-Attention: 문장 내 각 단어가 다른 단어들과의 관계를 고려해 더 풍부한 표현을 계산해 내는 방법.                
![photo 150](/assets/img/blog/img150.png)                  
* Self-Attention mechanism
  1. 각 입력 단어에 대해 쿼리(Query), 키(Key), 값(Value)이라는 3개의 벡터를 생성한다.
  2. Query와 Key vector 사이의 내적을 계산하여 각 단어 간의 유사도를 평가한다. 해당 유사도는 단어 간 관계를 측정하는 척도다.
  3. 계산된 유사도를 softmax 함수로 정규화하여 attention weight를 구한다. 이 가중치의 합은 1이 되며, 중요한 단어에 더 많은 가중치를 부여한다.      
  ![photo 151](/assets/img/blog/img151.png)                
  5. Value vector와 attention weight를 곱한 하여 context vector를 생성한다. context vector는 각 단어가 문장에서 가지는 의미와 관계를 반영한 dynamic expression을 제공한다.

### 4.2 Multi-Head Attention
* Multi-Head Attention: self attention을 여러 번 적용해, 입력 문장의 단어들에 대해 다양한 표현을 얻을 수 있게 해주는 방법으로 각 헤드(Head)마다 Query(Q), Key(K), Value(V)를 계산하여 이를 병렬로 처리한다.                   
![photo 152](/assets/img/blog/img152.png)                 
* Multi-Head Attention mechanism
  1. 각 입력 단어에 대해 학습된 maxtrix(W_Q, W_K, W_V)를 사용해 각 Head마다 다른 Query(Q), Key(K), Value(V) vector를 계산한다.        
  ![photo 153](/assets/img/blog/img153.png)            
  3. 해당 vector를 이용해 Attention값을 계산한다.
  4. head 간의 연산은 병렬로 수행되면서 각각의 head가 독립적으로 context에 맞는 표현을 생성할 수 있다.
  5. 각 head에서 계산된 attention값을 결합하고, 최종적으로 W_0을 곱해서 multi-head-attention 결과를 생성한다.

### 4.3 Transformer Network
* Transformer Network: 입력 문장을 encoding하여 출력 문장을 decoding 하는 구조를 가진다. attention mechanism을 통해 병렬처리가 가능하기 때문에 기존 순차적 모델의 한계를 극복한 모델이다.           
![photo 154](/assets/img/blog/img154.png)         
* encoder-decoder architecture
  1. encoder는 입력 문장을 처리하여 각 단어에 대한 특징을 계산하여 decoder에 전달한다.
  2. decoder는 번역을 생성하며, 이전에 생성한 단어와 encoder의 출력을 바탕으로 다음 단어를 예측한다.
* Muti-head attention
  1. 여러 개의 attention head를 사용해 각각 다른 관점에서 문장의 단어들에 집중하고 병렬로 처리한다.
  2. attention mechanism을 통해 입력 문장의 각 단어 간 관계를 고려하여 표현을 다양하게 만든다.
* Positional Encoding
  1. Transformer는 문장의 순서 정보를 반영하기 위해 sin, cos 함수를 사용한 Positional Encoding을 추가한다.
  2. 이를 통해 단어의 위치를 잘 구분할 수 있다.
* Residual Connection & Batch Normalization
  1. 잔여 연결을 통해 각 layer의 출력이 손실되지 않게 안정적인 학습을 진행한다.
  2. 배치 정규화를 통해 학습 속도와 성능을 향상시킨다.
* Mask Multi-Head attention
  1. 훈련 과정에서 미래의 단어를 참조하지 못하도록 mask를 적용하여, decoder가 올바른 순서로 단어를 생성하도록 만든다.
<br>      

참고문헌: C5W3 & C5W4

---
layout: single
title:  "Sequence Models & Attention Mechanism & Transformer Network"
---
## 3. Sequence Models & Attention Mechanism

### 3.1 Basic Models
* Seq2Seq Model: input sequence를 encoder network를 통해 처리하여 encoding된 vector를 decoder network로 전달하여 output sequence를 생성하는 신경망을 훈련시킨 모델.                
![photo 140](/assets/img/blog/img140.png)              
* encoder network: input sequence를 받아 벡터로 encoding한다. ex) RNN, GRU, LSTM 등
* decoder netwrok: encoding된 vector를 기반으로 한 단어씩 output sequence를 생성한다. ex) 영어 번역
* 이미지에서 추출한 feature vector를 Seq2Seq Model의 input으로 사용하면, 이미지에 대한 설명을 생성하는 image captioning에 활용할 수 있다.                
![photo 141](/assets/img/blog/img141.png)             
* 번역과 image captioning 모두 Seq2Seq 구조를 사용하지만, 번역에서는 무작위 출력보다는 가장 가능성이 높은 번역을 선택해야 하는 경우가 많다.

### 3.2 Picking the Most likely Sentence
* machine translation model은 conditional language model로, 입력된 문장을 조건으로 하여 출력될 문장의 확률을 모델링하는 방식이다.
* language model은 주어진 문장의 확률을 추정하여 문장을 생성하는데 사용되지만, machine translation에서는 입력된 문장에 대한 대응 번역을 조건부 확률로 계산하여 가장 적합한 번역을 찾아낸다.               
![photo 142](/assets/img/blog/img142.png)               
* machine translation model은 input sequence를 encoder로 처리한 encoding vector를 decoder network로 전달하여 output sequence를 생성한다. 이는 language mode과 유사하지만, 항상 zero vector로 시작하는 대신 입력 문장의 encoding된 표현을 사용한다.
* conditional language model에서는 가장 확률이 높은 출력을 찾는 것이 중요하므로 한 번에 하나의 가장 높은 확률의 단어를 선택하는 greedy search 방식 보다는 beam search algorithm을 활용해 조건부 확률을 극대화 하는 translation을 찾는다.              
![photo 143](/assets/img/blog/img143.png)         

### 3.3 Beam Search
* Beam Search algorithm: 각 단계에서 여러 가능성을 동시에 고려하여 가장 확률이 높은 단어들을 추적해 나가는 탐색 방법이다.           
![photo 144](/assets/img/blog/img144.png)                 
* Bean Search algorithm 작동 방식
  1. 입력 문장에 대해 각 단어의 확률을 계산하고 가장 확률이 높은 상위 B개의 단어를 선택한다. B는 빔 너비(폭)으로 동시에 고려하는 후보 문장의 수를 결정하는 값이다.
  2. 첫 번째 단어로 선택된 각 단어에 대해 두 번째 단어의 확률을 계산한다. 이후 두 단어의 조합 중에서 상위 B개의 쌍을 선택한다.
  3. 선택된 단어 쌍을 기반으로 세 번째 단어의 확률을 계산하고 상위 B개의 조합을 계속 추적한다. 이와 같은 방식으로 문장을 확장해 나아가면서 가능성이 높은 조합을 찾아간다.
  4. 문장의 끝(EOS) 기호가 나올 대까지 1~3을 반복한다. 아후 가장 성능이 좋은 번역을 출력한다.

### 3.4 Refinements to Beam Search
* Beam Search algorithm 단점: 문장이 길어질수록 더 많은 단어를 포함하므로 여러 확률을 곱하게 되어 최종 확률값이 매우 작아지는 문제가 발생한다.
* 해당 문제를 보완하기 위해 Length normalization 사용한다.
* Beam Search에서의 확률은 여러 단어의 조건부 확률을 곱한 값이므로 수치적으로 매우 작은 값을 다룬다. 이를 해결하기 위해 로그 확률을 취해 안정적인 계산 결과를 얻을 수 있다.             
![photo 146](/assets/img/blog/img146.png)            
* Length normalization: 로그 확률을 문장의 길이로 나누어 정규화 하는 방법             
![photo 145](/assets/img/blog/img145.png)      
* beam 너비(폭)을 의미하는 B값이 커질수록 더 많은 가능성을 고려하게 되어 성능이 좋아질 수 있지만, 계산 비용이 증가하고 메모리 소모가 커진다. 또한, 어느 정도 이상으로 증가시키면 성능도 크게 증가하지 않는다.
* 일반적으로 B값은 3에서 10정도로 적절한 값을 설정한다.

### 3.5 Error Analysis in Beam Search
* Error Analysis in Beam Search : beam search는 여러 가능성 중 상위 B개의 번역을 고려하지만 때때로 잘못 번역을 선택할 수 있다.
* beam search의 결과가 사람 번역(y^*)보다 낮은 품질의 번역(y-hat)을 생성했을 때, 두 가지 오류 가능성을 고려해야 한다.
  1. beam search 자체의 문제
  2. RNN model의 성능 문제
* Error Analysis 과정
  1. P(y^*|X)와 P(y-hat|x)를 계산하여 사람의 번역과 모델이 선택한 번역의 확률을 비교한다.
  2. beam search error = P(y*) > P(y-hat)
  3. RNN model error = P(y*) <= P(y-hat)
* Error Analysis을 통한 개선
  1. beam search error: B를 늘리거나 검색 알고리즘을 개선한다.
  2. RNN model error: model architecture를 개선하거나 더 많은 데이터로 학습시킨다.

### 3.6 Attention Model Intuition
* 기존 encoder-decoder 구조는 전체 문장을 한 번에 encoding하고 translation을 생성하는 방식이기 때문에 긴 문장에서는 성능이 떨어진다.     
![photo 147](/assets/img/blog/img147.png)          
* Attention Model: encoder의 입력 중에서 decoder가 특정 출력 단어를 생성할 때, 중요한 정보에 집중할 수 있도록 가중치를 부여하는 모델로 긴 문장을 처리할 때도 성능 저하를 방지한다.
* encoder-decoder architecture: 입력 문장을 encoder로 처리하여 각 단어를 생성하고 decoder를 통해 문장의 특정 부분에 집중해 번역을 생성한다.
* attention weight: 각 단어를 번역할 때의 가중치
* context vector: attention weight를 바탕으로 문장의 특정 부분에 집중하여 RNN translation을 생성한다.

### 3.7 Attention Model
* Attention Model은 출력 단어 생성 시 각 입력 단어에 가중치를 이용해 집중 정도를 다르게 설정한다.        
![photo 148](/assets/img/blog/img148.png)        
* Attention Model 작동 방식
  1. BRNN, GRU, LSTM을 사용하여 입력 문장의 각 단어에 대해 순방향 및 역방향 RNN을 사용해 각 단어에 대한 feature vector를 생성한다. 해당 feature vector는 각 단어의 의미와 주변 context 잘 표현한다.
  2. 출력 번역의 각 단어를 생성하는 과정에서, Decoder는 이전 단계의 은닉 상태와 입력 문장의 각 단어에 대한 feature vector를 기반으로 attention weight를 계산한다.
  3. 계산된 attention weight(α)는 softmax 함수로 정규화되어 가중치 합이 1이 되도록 한다.
  4. 최종적으로 attention weight(α)을 사용하여 입력 문장의 각 단어에 대한 집중도를 반영한 context vector를 생성한다. 해당 context vector를 decoder에 전달하여 다음 출력 단어를 생성하는 데 사용한다.
* attention weight(α)는 소규모 신경망을 통해 계산되고 경사 하강법을 사용해 훈련한다.
* attention weight(α) example                 
![photo 149](/assets/img/blog/img149.png)            

## 4. Transformer Network

### 4.1 Transformer Network Intuition

### 4.2 Self-Attention

### 4.3 Multi-Head Attention

### 4.4 Transformer Network

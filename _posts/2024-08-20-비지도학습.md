---
layout: single
title:  "비지도 학습"
---

## 5. 비지도 학습
### 5.1 군집 알고리즘
* 비지도 학습(unsupervised learning): data의 target이 없을 때 사용하는 머신러닝 알고리즘으로 패턴이나 구조를 발견할 때 사용된다.
* 비지도 학습의 종류: 군집(clustering), 차원 축소(PCA) 등
* 군집: 비슷한 샘플끼리 하나의 그룹으로 모으는 대표적인 비지도 학습
  * 군집 알고리즘으로 모은 샘플 그룹을 클러스터(cluster)라고 부른다.
<br>                   

### 과일 이미지 데이터로 군집 알고리즘 이해하기
```python
# 데이터 가져오기
!wget https://bit.ly/fruits_300_data -O fruits_300.npy

import numpy as np
import matplotlib.pyplot as plt

fruits = np.load('fruits_300.npy')

print(fruits.shape)
>>> (300, 100, 100)

print(fruits[0, 0, :])
>>>
[  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1
   2   2   2   2   2   2   1   1   1   1   1   1   1   1   2   3   2   1
   2   1   1   1   1   2   1   3   2   1   3   1   4   1   2   5   5   5
   19 148 192 117  28   1   1   2   1   4   1   1   3   1   1   1   1   1
   2   2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1    
   1   1   1   1   1   1   1   1   1   1 ]
```
* 해당 데이터셋은 kaggle에 공개된 사과, 바나나, 파인애플 흑백 사진 데이터다.
* 해당 데이터셋의 배열의 크기는 3차원이다.
* 첫 번재 차원(300)은 샘플의 개수를 의미한다.
* 두 번째 차원(100)은 이미지 높이를 의미한다.
* 세 번째 차원(100)은 이미지 너비를 의미한다.
* 따라서, 이미지 크기가 100 x 100인 과일 이미지 샘플이 300개 존재하는 것이다.
* 첫 번째 행에 있는 픽셀 100개 값의 출력 결과 해당 넘파이 배열은 흑백 사진을 담고 있어 0~255까지의 정숫값을 가지는 것을 알 수 있다.
<br>          

#### 과일 이미지 시각화 및 픽셀 분석
```python
# 원본 이미지
plt.imshow(fruits[0], cmap='gray')
plt.show()

# 흰색 검은색 반전 이미지
plt.imshow(fruits[0], cmap='gray_r')
plt.show()
```
![photo 61](/assets/img/blog/img61.png)
![photo 62](/assets/img/blog/img62.png)                       
* matplotlib의 imshow() 함수를 통해 넘파이 배열로 저장된 이미지를 쉽게 그릴 수 있다.
* imshow() 함수의 cmap 매개변수의 따라 그림이 달라질 수 있다.
  * cmap='gray' : 이미지를 흑백으로 나타내기 위한 설정
  * cmap='gray_r': 이미지를 흑백 반전으로 나타내기 위한 설정
    * 해당 그림의 경우 본래 검은 바탕에 사과 그림이 존재하나 흑백 반전을 통해 흰색 바탕에 사과 그림이 존재한다.
    * 'gray_r' 사용 이유: 다른 알고리즘에 사용될 때 픽셀값이 0이면 출력값도 0이 되어 의미가 없어진다. 픽셀값이 높아지면 출력값도 커지기 때문에 의미를 부여하기 좋아진다.
    * 픽셀값 0: 검은색 / 픽셀값 255: 흰색
    * 흑백 반전 시 흰색 바탕에 검은색 부분을 강조하여 사용할 수 있다.
* matplotlib의 subplots() 함수를 사용해 여러 개의 그래프를 배열처럼 그릴 수도 있다.
<br>           

#### 픽셀의 평균값을 이용한 시각화
```python
2차원 배열을 1차원으로 변경
apple = fruits[0:100].reshape(-1, 100*100)
pineapple = fruits[100:200].reshape(-1, 100*100)
banana = fruits[200:300].reshape(-1, 100*100)

# 과일 이미지 픽셀값의 평균값에 따른 시각화(히스토그램)
plt.hist(np.mean(apple, axis=1), alpha=0.8)
plt.hist(np.mean(pineapple, axis=1), alpha=0.8)
plt.hist(np.mean(banana, axis=1), alpha=0.8)
plt.legend(['apple', 'pineapple', 'banana'])
plt.show()
```
![photo 63](/assets/img/blog/img63.png)                   
* reshape() 메서드를 사용해 2차원의 이미지를 크기(100 x 100)를 1차원 배열(10,000)로 만든다. 첫 번째 차원을 -1로 지정하면 자동으로 남은 차원을 할당한다.
* 샘플의 픽셀 평균값을 계산해 히스토그램으로 시각화하면 과일에 따른 평균값의 분포를 알 수 있다.
  * axis=1: 열을 따라 계산한다.
  * axis=0: 행을 따라 계산한다.                     
    ![photo 66](/assets/img/blog/img66.png)                     
* 해당 히스토그램을 통해 평균값을 이용해 바나나는 구분해낼 수 있다.

### 픽셀별 평균값을 이용한 시각화
```python
# 픽셀별 평균값 시각화(1차원)
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].bar(range(10000), np.mean(apple, axis=0))
axs[1].bar(range(10000), np.mean(pineapple, axis=0))
axs[2].bar(range(10000), np.mean(banana, axis=0))
plt.show()

# 1차원 데이터를 2차원 데이터로 변경
apple_mean = np.mean(apple, axis=0).reshape(100, 100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)

# 픽셀별 평균값 시각화(2차원)
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, cmap='gray_r')
axs[1].imshow(pineapple_mean, cmap='gray_r')
axs[2].imshow(banana_mean, cmap='gray_r')
plt.show()
```
![photo 64](/assets/img/blog/img64.png)                 
![photo 65](/assets/img/blog/img65.png)                          
* 픽셀별 평균값(axis=0)을 계산해 bar chart 형식으로 나타나면 다음과 같은 특징을 알 수 있다..
  * 사과는 사진 아래쪽으로 갈수록 값이 높아진다.
  * 파인애플은 비교적 값이 고르고 높다.
  * 바나나는 중앙의 픽셀값이 높다.
* reshape() 메서드를 이용해 1차원 배열을 2차원 배열로 변경하면 이미지 데이터로 시각화가 가능하다.
* 2차원 이미지 시각화를 통해 과일별 이미지의 특징을 파악할 수 있다.

#### 평균값에 따른 이미지 클러스터링
```python
abs_diff = np.abs(fruits - apple_mean)
abs_mean = np.mean(abs_diff, axis=(1,2))
print(abs_mean.shape)


apple_index = np.argsort(abs_mean)[:100]
fig, axs = plt.subplots(10, 10, figsize=(10,10))
for i in range(10):
    for j in range(10):
        axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r')
        axs[i, j].axis('off')
plt.show()
```
![photo 67](/assets/img/blog/img67.png)             
* abs() 함수를 이용해 절댓값의 오차를 계산하고 평균값과 가까운 사진을 고르면 다음과 같은 결과가 나온다.
* 사과 사진 100개를 정확하게 고른 것을 알 수 있다.
* 이와 같이 비슷한 샘플끼리 그룹을 모으는 작업을 군집(clustering)이라고 한다.
* 하지만, 과일 이미지의 target 이름을 알고 있었기 때문에 완벽한 비지도 학습이라고 할 수는 없다.
<br>            

### 5.2 K-평균 군집 알고리즘
* K-Means 군집 알고리즘: 데이터를 K개의 군집으로 나누기 위해 각 데이터를 가장 가까운 군집 중심으로 할당하고, 군집 중심을 반복적으로 업데이트하여 군집을 최적화하는 비지도 학습 알고리즘이다.
* 클러스터 중심(cluster center) or 센트로이드(centroid): 클러스터의 평균값이 위한 곳을 의미한다.
* K-Means 알고리즘 작동 방식
  1. 무작위로 k개의 클러스터 중심을 정한다.
  2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플을 지정한다.
  3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경한다.
  4. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다.
  ![photo 68](/assets/img/blog/img68.png)             
* K-Means 알고리즘은 sklearn.cluster 모듈 아래 KMeans 클래스에 구현되어 있다.
* KMeans 매개변수
  * n_clusters: 클러스터의 개수를 지정한다. (기본값 8)
  * n_init: 반복 횟수 (기본값 10)
  * max_iter: 한 번의 실행에서 최적의 센트로이드를 찾기 위해 반복할 수 있는 최대 횟수(기본값 200)
<br>         

#### K-Meas 알고리즘을 이용한 클러스터링
```python
# 데이터 준비
!wget https://bit.ly/fruits_300_data -O fruits_300.npy

import numpy as np

# 3차원 데이터(300, 100, 100)를 2차원(300, 10,000)으로 변경
fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100)

# K-Means 클러스터링 진행
from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)
```
* K-Means 클래스 객체를 이용해 K-Means 클러스터링을 할 수 있다.
* 지도 학습과 다르게 비지도 학습이므로 fit 메서드에 target data를 사용하지 않는다.
<br>              

```python
# 군집 결과 출력
print(km.labels_)
>>> 
[2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 ]

# label 종류 및 개수 출력
print(np.unique(km.labels_, return_counts=True))
>>> (array([0, 1, 2], dtype=int32), array([111,  98,  91]))
```
* 군집된 결과는 KMeans 클래스 객체의 labels_ 속성에 저장된다.
* labels_ 길이는 샘플 개수로 각 샘플이 어떤 레이블에 해당되는지 나타낸다.
* unique 메서드를 이용해 label의 종류와 개수를 알 수 있다.
<br>            

```python
# 클러스터 중심 출력
print(km.cluster_centers_)

# 클러스터 중심까지 거리 변환
print(km.transform(fruits_2d[100:101]))
>>> [[3393.8136117  8837.37750892 5267.70439881]]

# 클러스 중심 예측
print(km.predict(fruits_2d[100:101]))
>>> [0]

# 반복 횟수 출력
print(km.n_iter_)
>>> 4
```
* kMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluster_centers_속성에 저장되어 있다.
* kMeans 클래스는 transform 메서드를 통해 훈련 데이터 샘플에서 클러스터 중심까지 거리를 변환해준다. 이는 특성값을 변환하는 도구로 사용할 수 있다.
  * 해당 코드는 0번 클러스터와 가까운 것을 알 수 있다.
* kMeans 클래스는 predict() 메서드를 통해 가장 가까운 클러스터 중심을 예측 클래스로 출력해준다.
  * 해당 코드는 0번 클러스터와 가장 가깝기 때문에 예측값도 0번을 출력한다.
* kMeans 알고리즘이 반복된 횟수는 n_iter 속성에 저장되어 있다.
  * 해당 코드는 클러스터 중심을 4번 옮기면서 최적의 클러스터를 찾은 걸 알 수 있다.
* 클러스터 중심을 특성 공학처럼 사용해 데이터셋을 저차원으로 변환할 수 있다.(알고리즘 속도 향상)
* 또는, 가장 가까운 거리에 있는 클러스터 중심을 샘플의 예측값으로 사용할 수 있다.
<br>              

#### 최적의 클러스터 K 찾기
* KMeans 알고리즘의 단점: 클러스터의 개수(K)를 미리 지정해야 한다.
* 최적의 K 값을 찾는 방법
  * 엘보우 방법: 클러스터 개수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터 개수를 찾는 방법
  * 이너셔(inertia): 클러스터 중심과 클러스터에 속한 샘플 사이의 거리 제곱 합
  * 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여 있는지를 나타내는 값으로, 클러스터 개수가 늘어나면 클러스터 개개의 크기는 줄어들기 때문에 이너셔는 감소한다.
  * 클러스터 개수를 증가시키면서 이너셔를 그래프로 그리면 감소하는 속도가 꺾이는 지점을 최적의 클러스터 개수로 정한다.                         
  ![photo 69](/assets/img/blog/img69.png)                          
<br>               

#### 이너셔 그래프 시각화
```python
inertia = []
for k in range(2, 7):
    km = KMeans(n_clusters=k, n_init='auto', random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_)

plt.plot(range(2, 7), inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
```
![photo 70](/assets/img/blog/img70.png)                                       
* KMeans 클래스는 자동으로 이너셔를 계산해서 inertia_ 속성으로 제공한다.
* 반복문을 통해 클러스터의 개수를 증가시킨 모델을 학습하고 해당 모델의 이너셔값을 리스트에 추가한 뒤, 해당 값을 그래프로 시각화하면 다음과 같은 그래프를 얻을 수 있다. 
* 해당 그래프는 k=3이 엘보우 지점이자 최적의 클러스터 개수다.
<br>                 

### 5.3 주성분 분석
* 차원 축소(dimensionality reduction): 데이터를 가장 잘 나타내는 일부 특성을 선택하여 데이터 크기를 줄이고 지도 학습 모델의 성능을 향상시킬 수 있는 방법으로 줄어든 차원에서 원본 차원으로 손실을 최대한 줄이면서 복원할 수도 있다.
* 차원 축소 알고리즘 종류: PCA(principal component analysis, 주성분 분석)
* 차원 축소의 필요성: 특성이 많으면 선형 모델의 성능이 높아지면서 trainin set에 대해 쉽게 과대적합될 수 있다. 이를 방지하기 위해 차원 축소 방법을 사용할 수 있다.
<br>          

* 주성분 분석(PCA): 데이터에 있는 분산이 큰 방향을 찾는 것.
* 분산: 데이터가 널리 퍼져있는 정도
* 2차원 데이터의 분산이 큰 방향            
  ![photo 71](/assets/img/blog/img71.png)                     
  * 길게 늘어진 대각선 방향이 분산이 가장 크다는 것을 알 수 있다.
  * 해당 직선을 원점 맞춘 벡터를 주성분이라고 한다.
* 주성분 벡터: 원본 데이터에 잇는 어떤 방향
* 주성분 벡터 특징
  1. 주성분 벡터의 원소 개수는 원본 데이터셋에 있는 특성 개수와 같다.
  2. 하지만, 원본 데이터는 주성분을 사용해 차원을 줄일 수 있다.
  3. 주성분은 가장 분산이 큰 방향이기 때문에 주성분에 투영하여 바꾼 데이터는 원본이 가지고 있는 특성을 가장 잘 나타낸다.
  4. 주성분은 원본 특성의 개수만큼 찾는다.
<br>               

#### PCA 클래스
* sklearn.decomposition 모듈 아래 PCA 클래스로 주성분 분석 알고리즘을 제공한다.
* PCA 매개변수
  * n_components: 주성분의 개수를 지정할 수 있다.
  * random_state: 넘파이 난수 시드값을 지정할 수 있다.
* PCA 속성
  * components_ : 훈련 세트에서 찾은 주성분이 저장된다.
  * explained_variance_ : 설명된 분산이 저장된다.
  * explained_variance_ratio_ : 설명된 분산의 비율이 저장된다.
<br>        

#### PCA 코드
```python
# 데이터 준비
!wget https://bit.ly/fruits_300_data -O fruits_300.npy

import numpy as np

fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100) # 3d -> 2d

from sklearn.decomposition import PCA

pca = PCA(n_components=50)
pca.fit(fruits_2d)

print(pca.components_.shape)
>>> (50, 10000)
```
* PCA 클래스 객체를 이용해 주성분 분석을 진행할 수 있다.
* 주성분 분석도 비지도 학습이므로 fit() 메서드에 타깃값을 제공하지 않는다.
* components_ 속성을 통해 50개의 주성분을 찾은 것을 알 수 있다.
* PCA 클래스에 주성분 비율을 입력하면 지정된 비율에 도달할 때까지 자동으로 주성분을 찾는다. (ex. n_components=0.5)
<br>           

```python
print(fruits_2d.shape)
>>> (300, 10000)

# 차원 축소
fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape)
>>> (300, 50)
```
* transform 메서드를 통해 10,000개의 특성을 50개로 줄인 것을 알 수 있다.
<br>            

```python
# 원본 데이터 재구성
fruits_inverse = pca.inverse_transform(fruits_pca)
print(fruits_inverse.shape)
```
* inverse_transform 메서드를 통해 원본 데이터의 특성을 복원할 수 있다.
* 50개의 특성이 분산을 가장 잘 보존하도록 변환된 것이기 때문에 가능하다.
<br>             

#### 설명된 분산
* 설명된 분산(explained variance): 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지를 기록한 값을 의미한다.
* PCA 클래스의 explained_variance_ratio 속성에 각 주성분의 설명된 분산 비율이 기록되어 있다.
* 첫 번째 주성분의 설명된 분산이 가장 크며 모든 분산 비율을 더하면 총 분산 비율을 얻을 수 있다.
<br>           

```python
# 설명된 분산비율 값의 총합
print(np.sum(pca.explained_variance_ratio_))
>>> 0.9215651897863715

# 설명된 분산 비율 시각화
plt.plot(pca.explained_variance_ratio_)
```
![photo 72](/assets/img/blog/img72.png)                   
* 50개의 특성으로 92%가 넘는 분산을 유지하고 있음을 알 수 있다.
* 설명된 분산의 비율을 그래프로 그려보면 적절한 주성분의 개수를 찾을 수 있다.
  * 해당 그래프를 통해 10개의 주성분이 대부분의 분산을 표현하고 있음을 알 수 있다.
<br>           

#### 원본 데이터와 차원 축소 데이터 성능 비교
```python
# 로지스틱 회귀 모델 불러오기
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

# target data 생성
target = np.array([0] * 100 + [1] * 100 + [2] * 100)

# 원본 데이터 성능 및 시간
from sklearn.model_selection import cross_validate

scores = cross_validate(lr, fruits_2d, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))
>>> 0.9966666666666667
>>> 1.819899892807007

# 차원 축소 데이터 성능 및 시간
scores = cross_validate(lr, fruits_pca, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))
>>> 1.0
>>> 0.032833099365234375
```
* 차원 축소 데이터를 사용해 정확도 향상과 학습 시간을 대폭 감소시킨 것을 알 수 있다.
<br>              

#### 차원 축소를 활용한 클러스터링
```python
# PCA 학습 진행
pca = PCA(n_components=0.5)
pca.fit(fruits_2d)

# 사용된 주성분의 개수
print(pca.n_components_)
>>> 2

# 원본 데이터 차원 축소 진행
fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape)
>>> (300, 2)

# 차원 축소 데이터로 성능 확인
scores = cross_validate(lr, fruits_pca, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))
>>> 0.9933333333333334
>>> 0.03713240623474121

# 차원 축소 데이터를 이용한 K-Means 클러스터링
from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_pca)

print(np.unique(km.labels_, return_counts=True))
>>> (array([0, 1, 2], dtype=int32), array([110,  99,  91]))
```
* PCA 클래스에 주성분 비율을 입력하면 지정된 비율에 도달할 때까지 자동으로 주성분을 찾는다.
* 2개의 특성만으로 원본 데이터에 있는 분산의 50%를 표현할 수 있고 성능은 정확도 99%를 달성했음을 알 수 있다.
* 차원 축소 데이터로 클러스터링한 결과 각각 91개, 99개, 110개의 샘플을 포함하고 있다. 원본 데이터를 사용했을 때와 비슷한 결과를 얻었다.
<br>                  

```python
# 클러스터링 결과를 산점도로 시각화
for label in range(0, 3):
    data = fruits_pca[km.labels_ == label]
    plt.scatter(data[:,0], data[:,1])
plt.legend(['apple', 'banana', 'pineapple'])
plt.show()
```
![photo 73](/assets/img/blog/img73.png)                     
* 훈련 데이터의 차원을 줄이면 시각화하기 쉽다.
* 2개의 특성을 사용했기 때문에 2차원으로 표현된 산점도 그래프로 시각화할 수 있다.
* 해당 그래프를 통해 2개의 특성만으로 각 과일을 잘 구분하고 있음을 알 수 있다. 
<br>                 

참고문헌: 혼자 공부하는 머신러닝+딥러닝 6장

---
layout: single
title:  "트리 알고리즘"
---

## 4. 트리 알고리즘
### 4-1 결정 트리
* 결정 트리(Decision Tree): 데이터를 잘 나눌 수 있는 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘
* 결정 트리 알고리즘의 이해 및 모델 특징
  * 불순도를 기준으로 정보 이득이 최대가 되도록 노드를 분할하는 알고리즘이다.
  * 마지막에 도달한 노드의 클래스 비율을 통해 예측을 만든다.
  * 다른 머신러닝 모델들보다 모델의 결과를 시각화하여 설명하기 쉽다.
  * 분류 시에는 DecisionTreeClassifer 클래스를 사용하여 리프 노드에 도달한 다수의 클래스를 예측값으로 사용한다.
  * 회귀 시에는 DecisionTreeRegressor 클래스를 사용하여 리프 노드에 도달한 샘플의 타깃을 평균하여 예측값으로 사용한다.
<br>                    

```python
# 데이터 준비
import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')
data = wine[['alcohol', 'sugar', 'pH']].to_numpy() 
target = wine['class'].to_numpy()]

# 데이터 나누기
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)

# 데이터 스케일링
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# 결정 트리 모델링
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target)) # 0.9969
print(dt.score(test_scaled, test_target)) # 0.8592
```
* Kaggle의 Red Wine Quality data를 활용한 Wine의 종류를 예측하는 분류 문제다.
* 분류 문제에 결정 트리 모델을 적용하기 위해 DecisionTreeClassifier 클래스를 사용했다.
* 결정 트리 모델링 방법은 다른 머신러닝 방법과 유사하다. (fit() 메서드 호출 후 score() 메서드로 정확도 평가)
* training set에 대한 score가 매우 높아 현재 결정 트리 모델은 과대 적합되었다.
<br>                  

#### 결정 트리 모델 시각화 및 해석
```python
# 결정 트리 모델 시각화
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![photo 53](/assets/img/blog/img53.png)                          
* plot_tree() 함수를 사용해 결정 트리를 이해하기 쉬운 트리 그림을 출력할 수 있다.
* 노드(node): 결정 트리를 구성하는 핵심요소로 훈련 데이터의 특성에 대한 테스트를 표현한다.
* 루트 노드(root node): 최상단 노드
* 리프 노드(leaf node): 최하단 노드
* 부모 노드(parent node): 상단 노드
* 자식 노드(child node): 하단 노드
* 가지(branch): 테스트 결과를 나타낸다.
* plot_tree() 함수 파라미터
  * max_depth: 트리의 깊이를 결정한다.
  * filled: 클래스에 맞게 노드의 색을 채운다. 클래스의 비율이 높아지면 점점 진한 색으로 표시한다.
  * feature_names: 특성의 이름을 전달한다.
<br>        

#### 노드 해석
![photo 54](/assets/img/blog/img54.png)                     
* 테스트 조건 : 샘플을 나누는 질문
* 불순도: 결정 트리가 최적의 질문을 찾기 위한 기준으로 결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이(정보 이득)가 가능한 크도록 트리를 성장시킨다. 노드를 순수하게 나눌수록 정보 이득은 커진다.
  * 정보 이득(information gain): 부모와 자식 노드 사이의 불순도 차이
  * 종류: 지니 불순도, 엔트로피 불순도
  * 지니 불순도(gini impurity) 계산 방법
  ![photo 55](/assets/img/blog/img55.png)                                        
  * 엔트로피 불순도(entropy impurity) 계산 방법
  ![photo 57](/assets/img/blog/img57.png)            
  * 불순도 차이(정보 이득) 계산 방법
  ![photo 56](/assets/img/blog/img56.png)                 
* 총 샘플수: 현 노드에 도달 샘플 수
* 클래스별 샘플 수: 현 노드에 도달한 샘플들의 클래스별 샘플 수
<br>              

#### 가지치기를 통한 결정 트리 모델의 과적합 방지 및 일반화
* 결정 트리를 깊이 제한 없이 성장하면 training set에 과대적합되기 쉽다.
* 결정 트리 모델은 트리의 최대 깊이를 미리 지정하는 가지치기를 통해 과적합을 방지할 수 있다.
* DecisionTreeClassifier 클래스의 max_depth 매개변수를 통해 가능하다.
```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)

print(dt.score(train_input, train_target)) # 0.8448
print(dt.score(test_input, test_target)) # 0.8415
```
* 가지치기를 통해 이전 모델보다 과적합을 방지할 수 있었다.
<br>            

#### 결정 트리 모델과 스케일링
* 결정 트리 모델은 표준화 전처리 과정이 필요가 없다.
* Why? 결정 트리 알고리즘은 클래스별 비율을 통해 계산한 불순도를 기준으로 샘플을 나누기 때문에 특성값의 스케일은 결정 트리 알고리즘에 영향을 미치지 않는다. 따라서 스케일링하지 않고 모델링을 진행해도 score값은 동일한 결과가 나오고 특성값을 표준점수로 바꾸지 않기 때문에 직관적으로 이해하기 쉽다.
<br>                     
 
### 결정 트리 모델의 특성 중요도
결정 트리는 해당 모델에서의 각 특성의 중요도를 계산해준다.
* 특성 중요도: 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값.
* 특성 중요도는 각 노드의 정보 이득과 전체 샘플에 대한 비율을 곱한 후, 특성별로 더하여 계산한다.
* 특성 중요도 활용을 통해 결정 트리 모델의 특성 선택에 활용 가능하다.
<br>              

```python
print(df.feature_importances_) # [0.12345626 0.86862934 0.0079144 ]
```
* 해당 결과를 통해 당도의 중요도가 가장 높은 것을 확인할 수 있다.
<br>

#### DecisionTreeClassifier 매개변수 정리
* criterion: 매개변수 불순도를 지정한다.
  * (기본값) 'gini': 지니 불순도를 의미한다.
  * 'entropy': 엔트로피 불순도를 의미한다.
* splitter: 노드블 분할하는 전략을 선택한다.
  * (기본값) 'best': 정보 이득이 최대가 되도록 분할한다.
  * 'random': 임의로 노드를 분할한다.
* max_depth: 트리가 성장할 최대 깊이를 지정한다.
  * (기본값) 'None': 리프 노드가 순수하거나 min_samples_split보다 샘플 개수가 적을 때까지 성장한다.
* min_samples_split: 노드를 나누기 위한 최소 샘플의 개수
  * (기본값) 2
* max_features: 최적의 분할을 위해 탐색할 특성의 개수를 지정한다.
  * (기본값) 'None': 모든 특성을 사용한다.
<br>

### 4.2 교차 검증과 그리드 서치









































































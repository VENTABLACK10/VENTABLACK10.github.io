---
layout: single
title:  "Object Detection & Face Recognition and Neural Style Transfer"
---

## 3. Object Detection

### 3.1 Object Localization
* Image classification: 보통 물체 한 개의 이미지를 분류하는 것
* Classification with localization: 보통 한 개의 물체 위치를 파악하여 해당 물체 주위에 경계 상자 그리는 것
* Object detection: 여러 물체의 위치를 감지하고 이에 대한 경계 상자를 그리는 것         
![photo 96](/assets/img/blog/img96.png)                      

* object localization target label y                     
![photo 97](/assets/img/blog/img97.png)                               
* p_c: 이미지 내 객체 존재 여부 (ex. 0 or 1,), 로지스틱 회귀 손실에 사용된다.
* b_x, b_y, b_h, b_w: 이미지 내 객체 중심점 좌표(x,y), 객체 높이, 객체 너비로 손실함수에는 제곱오차를 사용한다.
* c_1, c_2, c_3, ...: 객체로 분류될 수 있는 클래스로 클래스 중 하나에 대한 소프트 맥스 유닛의 출력을 손실함수로 사용한다.
* 이미지 내 객체가 있는 경우, p_c는 1을 가지고 객체와 동일한 클래스 값만 1을 가진다.             
![photo 98](/assets/img/blog/img98.png)          
* 이미지 내 객체가 없는 경우, p_c는 0을 가지고 나머지 값은 고려할 필요가 없으므로 무관항(NaN)을 가진다.                 
![photo 99](/assets/img/blog/img99.png)            

### 3.2 Landmark Detection
* Landmark Detection: 여러 개의 유닛을 추가해서 인식하고자 하는 특징점들의 각 좌표를 출력한다.
* Landmark Detection에서의 특징점은 다른 이미지에 대해서도 동일하게 적용해야 한다.        
![photo 100](/assets/img/blog/img100.png)                                   

### 3.3 Object Detection
* sliding windows detection : 전체 이미지에 대해서 고정된 크기의 window를 일정 간격으로 sliding 하면서, 각 위치에 객체가 존재하는지 합성곱 신경망 계산을 통해 감지하는 방법이다.        
![photo 101](/assets/img/blog/img101.png)        
* 단점: 이미지의 수 많은 영역을 모두 잘라내어 합성곱 신경망을 통해 계산해야 하므로 계산 비용이 많이 든다. 또한, 계산 비용을 줄이기 위해 sliding windows의 크기를 늘리면 성능이 저하될 수 있다.

### 3.4 Convolutional Implementation Sliding Windows
* sliding windows detection은 계산 비용을 줄이기 위해 합성곱 연산을 다음과 같이 사용한다.      
![photo 102](/assets/img/blog/img102.png)                
* 기존과 다르게 FC층을 만들기 전, padding 없이 filter 400개의 5 x 5 convolution을 통해 1 x 1 x 400인 FC(Fully Convolutional layer)을 만든다.          
![photo 103](/assets/img/blog/img103.png)                    
* Fully convolutional layer를 통해 겹치는(overlap) 정보를 공유하면서 빠른 연산 처리가 가능하다.
* 단점: 경계 상자의 위치가 정확하지 않을 수 있다

### 3.5 Intersection Over Union(IOU)
* Intersection over Union(IOU): localization의 Accuracy 성능을 평가하기 위해 사용되는 지표
* IoU를 구하는 방법: 두 경계 상자의 교집합 / 합집합        
![photo 104](/assets/img/blog/img104.png)                    
* IoU 특징
  1. 일반적으로 IoU값이 0.5보다 크면 괜찮다. (0.6, 0.7도 사용)
  2. 0.5는 임값으로 주로 사용한다.
  3. IoU값이 높을수록 경계상자는 더 정확하다.
  4. IoU = 0 : 예측 상자와 실제 상자가 전혀 겹치지 않는다.
  5. IoU = 0 : 예측 상자와 실제 상자가 완벽하게 겹친다.

### 3.6 Nonmax Suppression 
* Non-max Suppression: 동일한 객체에 대해 중복되거나 겹치는 경계 상자를 제거하여 최적의 경계 상자를 선택하는 알고리즘                             
![photo 105](/assets/img/blog/img105.png)                                            
* Non-max Suppression algorithm 과정
  1. 이미지 내에서 여러 객체의 경계 상자와 그에 대한 예측 확률을 출력한다.
  2. 가장 높은 예측 확률을 가진 경계 상자를 선택하고 해당 상자와 나머지 상자들의 IoU를 계산한다.
  3. IoU 값이 임계값(ex: 0.5)을 초과하면 중복 객체로 간주하고 제거한다.
  4. 남자 상자들 중에서 다음 예측 확률이 높은 상자를 선택하고 위의 과정을 반복한다.
* 탐지하고자 하는 객체가 여러 개인 경우 여러 개에 대해서 독립적으로 시행해준다.

### 3.7 Anchor Boxes
* Anchor Boxes: 격자 셀에 여러 개의 object를 감지하고 싶거나 겹쳐있는 객체를 모두 감지하고 싶을 때 사용하는 것으로 다양한 크기와 비율의 객체를 효과적으로 탐지할 수 있다.        
![photo 106](/assets/img/blog/img106.png)                             
* Anchor Boxes algorithm 과정
  1. 입력 이미지를 일정한 크기의 그리드로 분할
  2. 각 그리드 셀의 중심에 여러 개의 앵커 박스를 배치
  3. 각 앵커 박스마다 이와 같은 예측 값을 출력
  4. 각 앵커 박스와 실제 객체의 경계 상자 간 IoU 계산
  5. IoU가 가장 높은 앵커 박스를 해당 객체와 매칭
* 앵커 박스를 사용하면 네트워크가 여러 개체, 다른 크기의 개체, 그리고 겹치는 개체를 감지할 수 있다.

### 3.8 YOLO Algorithm
* YOLO Algorithm: 이미지를 한 번에 분석해 객체의 위치와 클래스(종류)를 동시에 빠르게 예측하는 객체 탐지 알고리즘.
* YOLO 알고리즘은 이미지를 한 번에 처리하여 객체를 검출하므로 속도가 빠르고 실시간 처리에 적합하다.          
![photo 107](/assets/img/blog/img107.png)               
* YOLO Algorithm 과정
  1. 입력 이미지를 특정 크기의 그리드 셀로 나눈다.
  2. 각 그리드 셀은 여러 개의 bounding box(경계 상자)를 예측한다.
  3. 각 그리드 셀은 객체가 특정 클래스에 속할 확률을 예측한다.
  4. 경계 상자와 클래스 확률을 기반으로 Non-Max Suppression을 사용해 중복된 경계상자를 제거하면서 최종 객체의 위치와 클래스를 검출한다.
 
### 3.9 Region proposals
* R-CNN(Region-CNN): component classifier를 실행할 지역을 고르는 영역 제안 단계와 제안된 영역의 window만 골라서 component classifier를 실행하는 영역 분류 단계를 거쳐 이미지를 처리하는 알고리즘.
<br>        

* Fast R-CNN: R-CNN의 느린 처리 속도를 개선한 알고리즘으로 이미지 전체를 한 번만 CNN을 적용하여 feature map을 추출하고 해당 feature map에서 region proposal을 추출한다.
* Fast R-CNN은 Region proposal을 위한 clustering 단계가 여전히 느리다는 단점이 있다.
<br>          

* Faster R-CNN: Fast R-CNN을 개선한 알고리즘으로 분할 알고리즘 대신 CNN을 사용해 이미지 특징 추출하고 객체가 있을 가능성이 높은 영역을 제안한다. 
* Faster R-CNN 기존 R-CNN 보다 속도와 성능이 개서뇌었지만, YOLO보다는 느리다.     
![photo 108](/assets/img/blog/img108.png)              

## 4. Face Recognition and Neural Style Transfer

### 4.1 One Shot Learning
* One Shot Learning: 소량의 데이터(1개) 샘플만으로 새로운 클래스를 인식하는 방법이다.
* One Shot Learning은 샘플의 개수가 적어 학습이 어렵기 때문에 유사도 함수를 학습해 사용하다.
* 유사도 함수 학습은 신경망에서 두 이미지간 차이를 반환하는 함수를 학습하는 것이다.
* 만약, 두 이미지가 같다면 작은 숫자를 반환하고 다르다면 큰 숫자를 반환한다.
* 따라서, 반환값이 임계값보다 작으면 같은 사람이라고 예측하고 크다면 다른 사람이라고 예측한다.    
![photo 109](/assets/img/blog/img109.png)    

### 4.2 Siamese Network
* Siamese Network: 두 개의 동일한 신경망 구조를 사용해 유사도를 비교하는 모델.
* Siamese Network는 두 개의 입력 이미지를 각각 동일한 신경망을 통해 처리한 후, 출력되는 특징 벡터들의 차이를 계산해서 유사도를 측정한다.
* 출력되는 특징 벡터들을 로지스틱 회귀 유닛을 이용하면 binary classification으로 활용할 수 있다.
* Siamese Network에서는 동일한 신경망을 사용하므로 동일한 가중치와 구조를 공유한다.
* 만약, 입력되는 두 이미지가 비슷하다면 유사도는 작은 값을 가지고 다르다면 유사도는 큰 값을 가진다.       
![photo 110](/assets/img/blog/img110.png)        
 
### 4.3 Triplet Loss
* Triplet Loss: 입력 데이터를 Anchor, Positive, Negative로 구분해 나누어 처리하는 손실 함수
* Triplet Loss 손실함수 식            
  ![photo 111](/assets/img/blog/img111.png)               
  ![photo 112](/assets/img/blog/img112.png)                   
  * Anchor와 Positive 사이 거리가 Anchor와 Negative 사이 거리보다 크다면 손실 발생
  * 반대로, Anchor와 Positive 사이 거리가 Anchor와 Negative 사이 거리보다 작다면 0 출력(손실 X)
  * 따라서 Anchor와 Positive 사이 거리는 최소화 되어야 하며 Negative와의 거리는 최대가 되어야 한다.
  * Anchor(A): 기준 데이터
  * Positive(P): Anchor와 같은 클래스의 데이터
  * Negative(N): Anchor와 다른 클래스의 데이터
  * α: positive와 negative 거리 차이를 보정하기 위한 margin(hyperparameter)
  * f: embedding

### 4.4 Neural Style Transfer Cost Function
* Neural Style Transfer: content image, style image을 사용해 한 이미지의 스타일을 다른 이미지에 적용하는 기술             
![photo 113](/assets/img/blog/img113.png)          
* Content Image(C): 기본적인 구조와 형태를 가진 이미지
* Style Image(S): content image에 적용하고자 하는 스타일의 이미지
* Generated Image(G): Style Image를 Content Image에 적용하여 생성된 이미지
* Neural Style Transfer Cost Function             
  ![photo 114](/assets/img/blog/img114.png)      
  * α: content weight
  * 𝛽: style weight
  * J_content(C,G): Content Cost Function
  * J_style(S,G): Style Cost Function

### 4.5 Content Cost Function
* Content Cost Function(J_content(C,G)): content image와 generated image의 유사도             
  ![photo 115](/assets/img/blog/img115.png)         
  * L_content(C,G): content loss function의 값으로 생성된 이미지(G)와 콘텐츠 이미지(C)의 유사도를 나타낸 수치로 값이 작을수록 두 이미지는 유사하다는 의미다.
  * A^(C)_ij: content image(C)의 CNN layer에서 추출한 feature map
  * A^(G)_ij: generated image(G)의 CNN layer에서 추출한 feature map
  * sigma_ij: CNN layer에서 추출한 feature map의 각 픽셀 i, j에 대한 합으로 모든 픽셀의 차이에 대한 합을 계산한다.
  * 1/2: Content Cost Function를 간결하게 만들기 위한 상수로 경사하강법을 통해 최적화할 때 미분을 용이하게 만들어준다.
* Content Cost Function은 content image와 generated image의 feature map을 비교한다.
* feature map이란 CNN에서 입력 이미지를 여러 층을 거치며 추출된 중간 표현으로 윤곽과 형태와 같은 이미지의 고수준 특성을 담고 있다.

### 4.6 Style Cost Function
* Style Cost Function: J_style(S,G): style image(S)와 generated image(G)의 유사도로 두 이미지의 Gram Matrix를 비교하여 계산한다.
* Style Cost Function 정의              
  ![photo 116](/assets/img/blog/img116.png)            
  * L_style(S,G): style cost function의 값으로 스타일 이미지(S)와 생성된 이미지(G)의 유사도
  * w_i: 각 layer i의 가중치로 각 layer가 스타일에 기여하는 정도를 조정하는 hyperparameter
  * E_i: 특정 layer i에서 계산된 style cost function 값
* 각 layer에서의 Style Cost(E_i) 정의              
  ![photo 117](/assets/img/blog/img117.png)           
  * G^(S)_kl: style image(S)의 CNN layer i에서 추출된 gram matrix의 k, l번째 component
  * G^(G)_kl: generated image(G)의 CNN layer i에서 추출된 gram matrix의 k, l번째 component
  * N_i: layer i에서 filter(channel) 개수
  * M_i: layer i에서 feature map의 가로 x 세로 픽셀 수
* Gram Matrix(그램 행렬): feature map의 상호 상관관계를 나타내는 행렬                
  ![photo 118](/assets/img/blog/img118.png)       
  * G^(i)_kl: CNN layer i에서 필터 k와 l 사이의 상관관계를 나타내는 그램 행렬의 component
  * A^(i)_kl: CNN layer i에서 필터 k가 이미지에서 추출한 feature map의 m번째 component
  * sigma_m: 모든 픽셀에 대해서 합한다.
  * Gram_Matrix는 특정 layer의 feature map에서 각 filter 간의 내적을 계산하여 style을 추출한다.
<br>             
 
참고문헌: C4W3 & C4W4


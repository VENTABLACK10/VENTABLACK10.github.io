---
layout: single
title:  "다양한 분류 알고리즘"
---

## 3. 다양한 분류 알고리즘
### 3.1 KNN 분류기의 클래스 확률 예측
![photo 44](/assets/img/blog/img44.png)
* KNN 분류기는 클래스 비율을 확률로 계산하여 제공한다.                  
<br>               

```python
# 데이터 준비
import numpy as np
fish = pd.read_csv('https://bit.ly/fish_csv_data')

# input data와 taeget data 분리
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
```
* 'Species': 생선 종류의 대한 feature로 target feature
* 'Weight','Length','Diagonal','Height','Width': 생선의 특성을 담은 input feature
* to_numpy() 메서드를 통해 데이터프레으로 된 데이터를 numpy 배열로 변경                  
<br>

```python
# training set, test set 분리
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)

# data scaling
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```
* train_test_split() 메서드를 통해 training set과 test set으로 분리
* StandardScaler 클래스를 사용하여 training set과 test set 표준화 전처리
* 전처리 주의점: training set의 통계값으로 test set을 변환한다.            
<br>               

```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))
```
* KNeighborsClassifier 클래스 객체 생성 후 training set으로 모델 학습 진행(fit)
* predict_proba() 메서드를 통해 클래스별 확률값을 반환한다.
  * 이진 분류의 경우 -> 샘플마다 음성 클래스와 양성 클래스에 대한 확률 반환
  * 다중 분류의 경우 -> 샘플마다 모든 클래스에 대한 확률을 반환
* round(): decimals 매개변수로 소수점 자릿수를 지정하여 반올림한다.             
<br>

```python
print(kn.classes_)
>>> ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']

print(kn.predict(test_scaled[:5]))
>>> ['Perch', 'Smelt', 'Pike', 'Perch', 'Perch']
```
* classes_ 속성을 통해 알파벳 순으로 정렬된 target의 종류를 알 수 있다.
* predict() 메서드를 통해 target을 예측으로 출력한다.
* predict_proba() 결과 해석 방법(네 번째 샘플 예시)           
![photo 45](/assets/img/blog/img45.png)         
  * 출력값에 대한 순서는 classes_ 속성과 동일하다.
  * 해당 샘플의 경우 'Perch'에 대법
 
### 3.2 로지스틱 회귀
* 로지스틱 회귀: 선형 방정식을 사용한 분류 알고리즘
* 로지스틱 회귀 특징: Sigmoid 또는 Softmax 함수를 사용하여 클래스 확률을 출력할 수 있다.                        
<br>               

#### 로지스틱 회귀 - 이진 분류
* Sigmoid Function
![photo 46](/assets/img/blog/img46.png)      
  * Sigmoid Function은 이진 분류 시에 사용되는 함수로 출력값이 0.5보다 크면 양성으로 0.5보다 작거나 같으면 음성으로 판단한다.
  * Sigmoid Function을 통해 선형 방정식의 출력 z값을 확률값으로 해석할 수 있다.
  * z값이 클수록 1에 가까워 지고 작을수록 0에 가까워 진다.                
<br>           

* 로지스틱 회귀(이진 분류)
<br>

```python
# 이진 분류를 위한 데이터 골라내기
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```
* 기존 데이터는 다중 분류를 위한 데이터 이므로 boolean indexing을 통해 도미, 빙어 데이터만 골라낸다.
* boolean indexing으로 도미 빙어에 대한 값은 True, 그 외의 값은 False로 반환된다.
<br>             

```python
# 선형 회귀 모델 학습
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```
* 로지스틱 회귀모델인 LogisticRegression 클래스 객체 생성 후, training set을 통해 학습한다.(fit)
* LogisticRegression: 선형 분류 알고리즘으로 로지스틱 회귀를 위한 클래스
* penalty 매개변수: l1(라쏘 규제), l2(릿지 규제, 기본값)
<br>

```python
# target class 예측 및 확률
print(lr.predict(train_bream_smelt[:5]))
>>> ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']

print(lr.predict_proba(train_bream_smelt[:5]))

# 클래스 속성 확인
print(lr.classes_)
```
* predict() 메서드를 통해 예측값을 반환한다.
* predict_proba() 메서드를 통해 샘플에 대한 클래스별 예측 확률을 반환한다.
* classes_ 통해 클래스 속성에 대해 확인할 수 있다.             
<br>

```python
# 계수 및 절편 확인 
print(lr.coef_, lr.intercept_)

# z값
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)
>>> [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]

# 시그모이드 함수를 통한 확률값 확인
from scipy.special import expit

print(expit(decisions))
>>> [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
```
* 선형회귀와 마찬가지로 로지스틱 회귀 또한 coef_ 와 intercept_ 통해 로지스틱 모델이 학습 선형 방정식의 계수값과 절편값을 확인할 수 있다.
* LogisticRegression 클래스는 decision_function 메서드를 통해 양성 클래스에 대한 z값을 출력할 수 있다.
* 출력된 z값을 scipy 라이브러리의 sigmoid 함수인 expit() 메서드를 통해 확률값을 계산할 수 있다.
* expit() 메서드로 계산된 확률값은 predict_proba() 메서드의 출력결과와 동일하다.
<br>

#### 로지스틱 회귀 - 다중 분류
* Softmax Function
  * 다중 분류 시에 사용되는 함수로 여러 개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체 합이 1이 되도록 만든다.
  * 다중 분류는 클래스마다 z값을 하나씩 계산한다.
* Softmax 계산 과정
  * 1. z1 ~ z7까지 값을 지수함수에 적용하고 모두 더한다.
  * 2. 각 지수함수 값을 전체합으로 나누어 준다.
![photo 47](/assets/img/blog/img47.png)             

* 로지스틱 회귀(다중 분류)
<br>

```python
# 다중 회귀 학습
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
```
* LogisticRegression 클래스는 기본적으로 반복적인 알고리즘을 사용한다.
* max_iter 매개변수를 통해 반복 횟수를 지정할 수 있다. (기본값 100)
* 충분한 훈련을 위해 max_iter 값을 늘린다.
* C 매개변수를 통해 규제를 제어한다.
* C는 선형회귀의 alpha 매개변수와 반대로 C가 작을수록 규제가 커진다. (기본값 1)
<br>

```python
# class 예측
lr.predict(test_scaled[:5])

# class별 확률값 
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
>>> [[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
     [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
     [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
     [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
     [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]
```
* predict() 메서드를 통해 샘플에 대한 예측값을 출력할 수 있다.
* predict_proba() 메서드를 통해 샘플에 대한 확률값을 출력할 수 있다.
* 가장 높은 확률 값이 해당 샘플의 예측 클래스로 출력된다.
<br>

```python
# 다중 분류의 선형 방정식 형태
print(lr.coef_.shape, lr.intercept_.shape)
>>> (7, 5) (7,)

# z값
decision = lr.decision_function(test_scaled[:5])

# 소프트맥스 함수를 통한 확률값 확인
from scipy.special import softmax

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```
* shape() 함수를 통해 z값이 7개가 계산된다는 것을 알 수 있다.
* decision_function() 메서드를 통해 z1~z7 값을 구한다.
* sicpy의 소프트맥스 함수인 softmax() 함수를 이용해 7개의 z값을 확률로 변환한다.
* softmax()의 axis 매개변수는 소프트맥스를 계산할 축을 지정한다.
  * axis=1 : 각 샘플(행)에 대한 소프트맥스를 계산한다.
  * 축을 지정하지 않으면, 배열 전체에 대한 소프트맥스를 계산한다.          
<br>              

### 3.3 확률적 경사 하강법

---
layout: single
title:  "다양한 분류 알고리즘"
---

## 3. 다양한 분류 알고리즘
### 3.1 KNN 분류기의 클래스 확률 예측
![photo 44](/assets/img/blog/img44.png)
* KNN 분류기는 클래스 비율을 확률로 계산하여 제공한다.                  
<br>               

```python
# 데이터 준비
import numpy as np
fish = pd.read_csv('https://bit.ly/fish_csv_data')

# input data와 taeget data 분리
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
```
* 'Species': 생선 종류의 대한 feature로 target feature
* 'Weight','Length','Diagonal','Height','Width': 생선의 특성을 담은 input feature
* to_numpy() 메서드를 통해 데이터프레으로 된 데이터를 numpy 배열로 변경                  
<br>

```python
# training set, test set 분리
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)

# data scaling
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```
* train_test_split() 메서드를 통해 training set과 test set으로 분리
* StandardScaler 클래스를 사용하여 training set과 test set 표준화 전처리
* 전처리 주의점: training set의 통계값으로 test set을 변환한다.            
<br>               

```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))
```
* KNeighborsClassifier 클래스 객체 생성 후 training set으로 모델 학습 진행(fit)
* predict_proba() 메서드를 통해 클래스별 확률값을 반환한다.
  * 이진 분류의 경우 -> 샘플마다 음성 클래스와 양성 클래스에 대한 확률 반환
  * 다중 분류의 경우 -> 샘플마다 모든 클래스에 대한 확률을 반환
* round(): decimals 매개변수로 소수점 자릿수를 지정하여 반올림한다.             
<br>

```python
print(kn.classes_)
>>> ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']

print(kn.predict(test_scaled[:5]))
>>> ['Perch', 'Smelt', 'Pike', 'Perch', 'Perch']
```
* classes_ 속성을 통해 알파벳 순으로 정렬된 target의 종류를 알 수 있다.
* predict() 메서드를 통해 target을 예측으로 출력한다.
* predict_proba() 결과 해석 방법(네 번째 샘플 예시)           
![photo 45](/assets/img/blog/img45.png)         
  * 출력값에 대한 순서는 classes_ 속성과 동일하다.
  * 해당 샘플의 경우 'Perch'에 대법
 
### 3.2 로지스틱 회귀
* 로지스틱 회귀: 선형 방정식을 사용한 분류 알고리즘
* 로지스틱 회귀 특징: Sigmoid 또는 Softmax 함수를 사용하여 클래스 확률을 출력할 수 있다.                        
<br>               

#### 로지스틱 회귀 - 이진 분류
* Sigmoid Function            
![photo 46](/assets/img/blog/img46.png)                      
  * Sigmoid Function은 이진 분류 시에 사용되는 함수로 출력값이 0.5보다 크면 양성으로 0.5보다 작거나 같으면 음성으로 판단한다.
  * Sigmoid Function을 통해 선형 방정식의 출력 z값을 확률값으로 해석할 수 있다.
  * z값이 클수록 1에 가까워 지고 작을수록 0에 가까워 진다.                
<br>           

* 로지스틱 회귀(이진 분류)
<br>

```python
# 이진 분류를 위한 데이터 골라내기
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```
* 기존 데이터는 다중 분류를 위한 데이터 이므로 boolean indexing을 통해 도미, 빙어 데이터만 골라낸다.
* boolean indexing으로 도미 빙어에 대한 값은 True, 그 외의 값은 False로 반환된다.
<br>             

```python
# 선형 회귀 모델 학습
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```
* 로지스틱 회귀모델인 LogisticRegression 클래스 객체 생성 후, training set을 통해 학습한다.(fit)
* LogisticRegression: 선형 분류 알고리즘으로 로지스틱 회귀를 위한 클래스
* penalty 매개변수: l1(라쏘 규제), l2(릿지 규제, 기본값)
<br>

```python
# target class 예측 및 확률
print(lr.predict(train_bream_smelt[:5]))
>>> ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']

print(lr.predict_proba(train_bream_smelt[:5]))

# 클래스 속성 확인
print(lr.classes_)
```
* predict() 메서드를 통해 예측값을 반환한다.
* predict_proba() 메서드를 통해 샘플에 대한 클래스별 예측 확률을 반환한다.
* classes_ 통해 클래스 속성에 대해 확인할 수 있다.             
<br>

```python
# 계수 및 절편 확인 
print(lr.coef_, lr.intercept_)

# z값
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)
>>> [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]

# 시그모이드 함수를 통한 확률값 확인
from scipy.special import expit

print(expit(decisions))
>>> [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
```
* 선형회귀와 마찬가지로 로지스틱 회귀 또한 coef_ 와 intercept_ 통해 로지스틱 모델이 학습 선형 방정식의 계수값과 절편값을 확인할 수 있다.
* LogisticRegression 클래스는 decision_function 메서드를 통해 양성 클래스에 대한 z값을 출력할 수 있다.
* 출력된 z값을 scipy 라이브러리의 sigmoid 함수인 expit() 메서드를 통해 확률값을 계산할 수 있다.
* expit() 메서드로 계산된 확률값은 predict_proba() 메서드의 출력결과와 동일하다.
<br>

#### 로지스틱 회귀 - 다중 분류
* Softmax Function
  * 다중 분류 시에 사용되는 함수로 여러 개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체 합이 1이 되도록 만든다.
  * 다중 분류는 클래스마다 z값을 하나씩 계산한다.
* Softmax 계산 과정
  1. z1 ~ z7까지 값을 지수함수에 적용하고 모두 더한다.
  2. 각 지수함수 값을 전체합으로 나누어 준다.
![photo 47](/assets/img/blog/img47.png)             

* 로지스틱 회귀(다중 분류)
<br>

```python
# 다중 회귀 학습
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
```
* LogisticRegression 클래스는 기본적으로 반복적인 알고리즘을 사용한다.
* max_iter 매개변수를 통해 반복 횟수를 지정할 수 있다. (기본값 100)
* 충분한 훈련을 위해 max_iter 값을 늘린다.
* C 매개변수를 통해 규제를 제어한다.
* C는 선형회귀의 alpha 매개변수와 반대로 C가 작을수록 규제가 커진다. (기본값 1)
<br>

```python
# class 예측
lr.predict(test_scaled[:5])

# class별 확률값 
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
>>> [[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
     [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
     [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
     [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
     [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]
```
* predict() 메서드를 통해 샘플에 대한 예측값을 출력할 수 있다.
* predict_proba() 메서드를 통해 샘플에 대한 확률값을 출력할 수 있다.
* 가장 높은 확률 값이 해당 샘플의 예측 클래스로 출력된다.
<br>

```python
# 다중 분류의 선형 방정식 형태
print(lr.coef_.shape, lr.intercept_.shape)
>>> (7, 5) (7,)

# z값
decision = lr.decision_function(test_scaled[:5])

# 소프트맥스 함수를 통한 확률값 확인
from scipy.special import softmax

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```
* shape() 함수를 통해 z값이 7개가 계산된다는 것을 알 수 있다.
* decision_function() 메서드를 통해 z1~z7 값을 구한다.
* sicpy의 소프트맥스 함수인 softmax() 함수를 이용해 7개의 z값을 확률로 변환한다.
* softmax()의 axis 매개변수는 소프트맥스를 계산할 축을 지정한다.
  * axis=1 : 각 샘플(행)에 대한 소프트맥스를 계산한다.
  * 축을 지정하지 않으면, 배열 전체에 대한 소프트맥스를 계산한다.          
<br>              

### 3.3 점진적 학습 & 경사하강법
* 점진적 학습(온라인 학습): 기존 데이터로 학습한 모델을 유지하면서 추가적인 새로운 데이터에 대해 조금씩 훈련 방법
* 점진적 학습 알고리즘: 확률적 경사 하강법(SGD)
<br>            

* 확률적 경사 하강법(Stochastic Graient Descent): training set에서 랜덤하게 하나의 샘플을 골라 학습하는 방법으로 신경망 알고리즘에 주로 사용된다.
* 확률 경사 하강법의 학습 과정
  1. Training set에서 랜덤하게 하나의 샘플을 선택하여 그래프의 접선의 기울기가 작아지는 방향으로 학습을 진행한다.
  2. 다음 training set에서 또 다른 샘플을 선택하여 동일 과정을 반복한다.
  3. 앞선 과정을 전체 샘플을 모두 사용할 때까지 반복한다.
  * 만약 그래프의 만족할만한 위치에 도달하지 못한 경우, training set에서 다시 랜덤하게 하나의 샘플을 선택해가면서 진행한다.
* 훈련 세트를 한 번 모두 사용하는 과정을 1 epoch라고 한다. (일반적으로 수십, 수백번 진행)
* 경사 하강법의 종류 정리
  1. 확률적 경사 하강법: 샘플을 1개씩 사용해 경사 하강법을 수행하는 방식
  2. 미니배치 경사 하강법: 여러 개의 샘플을 사용해 경사 하강법을 수행하는 방식
  3. 배치 경사 하강법: 한 번에 전체 샘플을 사용해 경사 하강법을 수행하는 방식
	  * 배치 경사 하강법은 전체 데이터를 사용하기 때문에 안정적이나 그만큼 컴퓨터 자원을 많이 사용한다. (주의)                       
![photo 48](/assets/img/blog/img48.png)                
<br>              

### 3.5 손실 함수
* 손실 함수(loss function): 확률적 경사 하강법이 최적화할 대상
* 손실함수 특징
  * 경사 하강법 사용을 위해 손실 함수는 미분 가능해야 한다.
<br>              

* 분류에서의 손실: 예측값과 정답(target)의 일치 여부를 통해 불일치시, 손실 증가
  * 이진 분류 손실함수: 로지스틱 회귀 손실 함수(양성 예측 확률 X 정답(타깃))      
  ![photo 49](/assets/img/blog/img49.png)             
  * 예측 확률에 로그 함수 적용 시 최종 손실 값을 양수로 확인 가능하다.
  * 또한, 로그 함수는 0에 가까울수록 음수값이 커지므로 손실을 크게 만들어 모델에 큰 영향을 미칠 수 있다.           
  ![photo 50](/assets/img/blog/img50.png)                        
  * 다중 분류 손실함수: 크로스엔트로피 손실함수(cross-entropy loss function)
* 회귀 손실 함수: 평균절대값오차(MAE), 평균 제곱 오차(MSE) 등 사용

### 3.6 경사 하강법을 사용한 모델
* 확률적 경사 하강법 사용 분류 모델: SGDClassifier
* 확률적 경사 하강법 사용 회귀 모델: SGDRegressor
<br>           

* SGDClassifier 매개변수
  * loss: 손실함수의 종류 지정
    * 기본값: ‘hinge’ -> hinge loss는 SVM 알고리즘을 위한 손실 함수
    * ‘log_loss’: 로지스틱 회귀를 위한 손실 함수
  * max_iter: 수행할 epoch 횟수 지정(기본값 1000)
    * ConvergenceWarning 경고 발생 시, max_iter 값을 늘린다. 모델이 충분히 수렴하지 않았다는 의미다.
  * penalty: 규제의 종류 지정
    * 기본값: ‘l2’ -> L2 규제 적용
    * ‘l1’ -> L1 규제 적용
  * alpha: 규제 강도 지정, (기본값 0.0001)
  * tol: 반복을 멈출 조건(기본값 0.001)
  * n_iter_no_change: 지정한 epoch 동안 손실이 tol만큼 줄어들지 않으면 알고리즘 중단(기본값 5)
<br>               

* SGDRegressor 매개변수
  * loss: 손실함수의 종류 지정
    * 기본값 ‘squared_loss’ -> 제곱 오차
  * 기타 매개변수는 SGDClassifier와 동일하게 사용된다.

* SGDClassifier 모델링
<br>                  

```python
# 데이터 준비 및 나누기, 스케일

import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')

fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```
* fish data를 가져와 train_test_split() 함수를 사용해 spliting을 진행하고 training set과 test set의 특성을 표준화 전처리를 진행한다.
<br>             

```python
sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(test_scaled, test_target)) # 0.775

sc.partial_fit(train_scaled, train_target)
print(sc.score(test_scaled, test_target)) # 0.85
```
* 확률적 경사 하강법을 제공하는 분류용 클래스 SGDClassifier 객체 생성
* 손실함수는 log_loss로 지정, 반복횟수 10회 지정
* fit() 메서드로 일반적인 학습을 진행하지만, partial_fit() 메서드로 점진적인 학습이 가능하다. 호출할 때마다 1 epoch씩 이어서 훈련이 가능하다.
* 점진적인 학습 결과 정확도가 상승한 것을 알 수 있다.
* train_scaled와 train_target을 한 번에 모두 사용하여 학습한 것처럼 보이지만, 해당 알고리즘은 전달한 training set에서 1개씩 샘플을 꺼내어 경사 하강법 단계를 수행한다.
* SGDClassifie는 미니배치 경사 하강법 또는 배치 하강법을 제공하지 않는다.

### 3.7 epoch 과대/과소 적합 해석
* 확률적 경사 하강법을 사용한 모델은 epoch 횟수에 따라 과소적합이나 과대적합이 발생할 수 있다.
* epoch 횟수가 적으면 모델이 training set을 덜 학습한다.
* epoch 횟수가 많으면 모델이 training set을 완전히 학습한다.               
![photo 51](/assets/img/blog/img51.png)             
* training set 점수는 epoch가 진행될수록 꾸준히 증가하지만, test set 점수는 어느 순간 감소하기 시작한다. (과대적합 시작 지점)
* 조기 종료(early stopping): 과대 적합이 시작하기 전에 훈련을 멈추는 것
<br>           

* epoch에 따른 성능 변화 그래프 시각화
<br>            

```python
import numpy as np

sc = SGDClassifier(loss='log_loss', random_state=42)

train_score = []
test_score = []

classes = np.unique(train_target)

# 점진적 학습에 따른 score 측정
for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)

    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))

# 시각화
import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```
![photo 52](/assets/img/blog/img52.png)             
* 백 번째 epoch 이후, training set과 test set의 점수가 벌어지는 것을 알 수 있다.
* epoch 초기에는 과소적합되어 training set과 test set의 점수가 낮다.
* 해당 모델의 경우 100번째 epoch가 적절한 반복 횟수다
<br>                

참고문헌: 혼자 공부하는 머신러닝+딥러닝 4장

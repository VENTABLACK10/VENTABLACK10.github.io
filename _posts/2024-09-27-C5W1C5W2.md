---
layout: single
title:  "RNN & NLP & Word Embeddings"
---
## 1. Recurrent Neural Networks

### 1.1 Recurrent Neural Network Model
* 기존 Neural Network Model의 한계: 표준 신경망은 입력과 출력값의 길이가 다를 수 있고 학습한 패턴을 공유하지 못해 sequence data 처리에 적합하지 않다. 또한, 입력 차원이 크기 때문에 매개변수가 매우 많아진다는 단점이 있다.
* Recurrent Neural Network(RNN): sequence data를 처리하기 위해 고안된 신경망으로 각 입력 시점에서 이전 시점의 정보를 전달받아 학습하기 때문에 입력 sequence의 연속적인 정보를 효율적으로 학습할 수 있다.    
![photo 119](/assets/img/blog/img119.png)            
* RNN 구조: 입력 시점마다 동일한 가중치 matrix를 사용하여 입력과 이전 시점의 상태 정보를 함께 처리한다. 이를 통해 매개변수 수가 크게 줄고 입력 데이터의 연속성을 반영한 학습이 가능하다.
* 각 시점에서 입력 벡터와 이전 활성화 값이 결합되어 새로운 활성화 값이 계산된다.
* 순전파 과정에서 새로운 활성화 값을 계산할 때, tanh, ReLU 활성화 함수가 활용된다.
* 출력값 예측에서는 이진 분류의 경우 Sigmoid, 다중 분류의 경우 Softmax를 사용한다.
* RNN 한계: 해당 RNN은 이전 시점의 정보만을 활용하기 때문에 Sequence 뒤쪽에 있는 정보를 사용하지 못하는 한계가 있다. -> 추후 이를 개선한 양방향 RNN인 BRNN 등장

### 1.2 Backpropagation through time
* 순전파(Forward-propagation): 입력 sequence를 사용하여 각 단계에서 은닉 상태를 계산하고 이를 통해 예측값을 출력하는 형태를 의미한다.
* 역전파(Back-propagation): 순전파의 반대 방향(오른쪽 -> 왼쪽)으로 계산하는 것으로 매개변수에 대한 기울기를 계산해 경사하강법으로 모델의 매개변수를 업데이트한다.      
![photo 120](/assets/img/blog/img120.png)       
* 손실 계산: 각 단계에서 예측값과 실제값의 차이를 기반으로 손실을 계산하고 전체 sequence의 손실은 모든 단계의 손실합으로 정의한다.

### 1.3 Different types of RNNs
* input sequence와 output sequence의 길이가 항상 같을 필요는 없다.
* RNN types
  1. One to one: 일반적인 신경망 구조로 입력값과 출력값이 하나인 경우
  2. One to many: 입력값은 하나이고 출력값이 sequence인 경우 / ex) 음악 생성
  3. Many to one: 입력값이 sequence이고 출력값이 하나인 경우 / ex) 감성 분류
  4. Many to many: 입력값과 출력값의 sequence가 동일한 경우(ex. 개체명 인식) or 다른 경우(ex.기계 번역)
     * 기계 번역처럼 입력과 출력 sequence가 다른 경우, RNN은 incoder와 decoder로 구성되어 입력 sequence를 처리한다.          
![photo 121](/assets/img/blog/img121.png)        
 
### 1.4 Language model and sequence generation
* language model은 단어 sequence를 입력으로 받아 그 sequence의 확률을 추정하기 위해 corpus를 사용해 학습한다. (corpus: 대규모 텍스트 데이터)
* language model은 문장을 단어(token) 단위로 나누고(tokenizer), 각 단어를 one-hot vector or index로 변환한다. 또한, 문장을 끝을 나타내기 위해 EOS token을 추가하거나 어휘에 없는 단어는 UNK(unknown token)로 처리한다.       
![photo 122](/assets/img/blog/img122.png)                
* RNN은 softmax loss function을 기반으로 학습하여 각 단계의 예측과 실제값 간의 차이를 최소화한다.

### 1.5 Sampling novel sequences
* Sampling을 통해 문장을 생성하는 방법:
  1. RNN language model은 학습 이후 소프트맥스 기반으로 단어를 샘플링하여 문장을 생성한다.
  2. 첫 번째 단어를 샘플링한 후, 이를 입력값으로 사용해 다음 단어를 예측한다. (반복을 통한 문장 생성)
  3. 문장이 끝날 때 EOS token을 생성하거나 미리 정해진 단어 수에 도달할 때까지 샘플링을 반복한다.
* 개별 문자를 예측하는 문자 수준의 RNN language model을 만들 수도 있다. 하지만, 더 긴 sequence를 처리해야 하고 계산 비용이 더 크다.
* 문자 수준의 언어 모델은 단어 수준의 언어 모델보다 긴 종속성을 처리하는 데 어려움이 있지만, 더 유연하게 작동한다. 그럼에도 대부분 단어 수준의 언어 모델을 사용한다.
  

### 1.6 Vanishing gradients with RNNs
* RNN은 이전 입력이 이후 출력에 영향을 미치는 장기적인 의존성을 잘 학습하지 못한다. (기억력 낮음)
* Vanishing gradients problem: 매우 긴 sequence를 처리할 때, 역전파 과정에서 값이 소멸하여 초기 입력의 정보가 후반부까지 전달되지 못하는 문제를 의미한다. 이로 인해 장기적인 패턴 학습이 어려워 진다. 
* Exploding gradient problem: 기울기가 기하급수적으로 커지면서 NaN 값을 발생시키는 문제로 gradient clipping을 통해 어느정도 해결 가능하다.        
![photo 123](/assets/img/blog/img123.png)               

### 1.7 Gated Recurrent Unit (GRU)
* GRU는 vanishing gradients problem을 해결하기 위해 나온 개념으로 장거리 의존성을 학습할 수 있도록 도와준다.                 
![photo 124](/assets/img/blog/img124.png)                     
* GRU 작동 원리
  1. 메모리 셀(c) 도입을 통해 중요한 정보를 기억하도록 한다.
  2. 게이트(Gamma_u) 도입을 통해 정보를 업데이트할 시점을 결정한다.
  3. GRU는 각 단계에서 후보 메모리 셀(c)을 계산한다.
  4. 게이트(Gamma_u)는 정보 업데이트 유무를 결정한다. 1이면 메모리 셀을 업데이트하고 0이면 기존 메모리 셀을 유지한다.
* Full GRU: 기존 GRU에 적절성 게이트(Gamma_r)를 추가한 것으로 이전 단계의 정보 반영 양을 결정한다.

### 1.8 LSTM (long short term memory) unit
* LSTM: GRU보다 더 복잡한 구조를 가진 RNN unit으로 vanishing gradients problem을 해결하고 장기적인 의존성을 잘 학습할 수 있다.
* LSTM은 GRU와 유사하게 메모리 셀(c_t)을 사용하지만, 더 많은 게이트를 사용하여 정보를 유연하게 처리한다.
* LSTM의 3가지 gate
  1. Update gate(Gamma_u): 새로운 정보 업데이트 유무 결정
  2. Forget gate(Gamma_f): 이전 메모리 셀 값을 유지량 결정
  3. Output gate: 메모리 셀의 출력 결정                
![photo 125](/assets/img/blog/img125.png)        
* LSTM 작동 방식: 메모리 셀(c_t)은 forget gate와 update gate에 의해 갱신되고 출력은 output gate에 의해 결정된다.
* GRU VS LSTM
  * GRU: 2개의 gate를 사용하고 단순하며 계산이 빠르다.
  * LSTM: GRU에 비해 복잡하지만 더 좋은 성능을 제공한다. 

### 1.9 Bidirectional RNN
* Bidirectional RNN(BRNN): 양방향 RNN으로 일반적인 정보 처리(왼쪽 -> 오른쪽)와 다르게 sequence의 양쪽 정보를 모두 활용하여 정보를 처리한다. 이에 더 정확한 예측이 가능하고 문장 내 단어의 의미를 잘 파악할 수 있다.           
![photo 126](/assets/img/blog/img126.png)        
* BRNN 작동 원리
  1. 두 개의 RNN Network(forward, back)를 사용하여 입력 sequence를 처리한다.
  2. 순방향 네트워크는 왼쪽에서 오른쪽으로, 역방향 네트워크는 오른쪽에서 왼족으로 정보를 처리한다.
  3. 순방향 네트워크와 역방향 네트워크의 정보를 결합해 예측을 생성한다.
* BRNN은 기존 RNN, GRU, LSTM과 함께 사용될 수 있다.
* BRNN은 전체 sequence가 필요하므로 실시간 처리에는 적합하지 않다.

### 1.10 Deep RNNs
* Deep RNN: RNN의 여러 계층을 쌓아 더 복잡한 sequence를 학습하고 처리위한 모델이다. 각 층은 이전 계층의 출력을 입력으로 받아 처리한다.
* 각 계층의 활성화 값은 이전 계층의 활성화 값과 함께 사용되며, 같은 매개변수 세트를 공유한다.     
![photo 126](/assets/img/blog/img126.png)        
* Deep RNN은 standard RNN, GRU, LSTM 기반으로 구성할 수 있으며 BRNN을 심층으로 만들 수 있다.
* Deep RNN은 훈련 비용이 많이 들지만 복잡한 패턴을 학습할 수 있다는 측면에서 유용하다.

## 2. Natural Language Processing & Word Embeddings

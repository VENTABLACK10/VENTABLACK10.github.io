---
layout: single
title:  "RNN & NLP and Word Embeddings"
---
## 1. Recurrent Neural Networks

### 1.1 Recurrent Neural Network Model
* 기존 Neural Network Model의 한계: 표준 신경망은 입력과 출력값의 길이가 다를 수 있고 학습한 패턴을 공유하지 못해 sequence data 처리에 적합하지 않다. 또한, 입력 차원이 크기 때문에 매개변수가 매우 많아진다는 단점이 있다.
* Recurrent Neural Network(RNN): sequence data를 처리하기 위해 고안된 신경망으로 각 입력 시점에서 이전 시점의 정보를 전달받아 학습하기 때문에 입력 sequence의 연속적인 정보를 효율적으로 학습할 수 있다.    
![photo 119](/assets/img/blog/img119.png)            
* RNN 구조: 입력 시점마다 동일한 가중치 matrix를 사용하여 입력과 이전 시점의 상태 정보를 함께 처리한다. 이를 통해 매개변수 수가 크게 줄고 입력 데이터의 연속성을 반영한 학습이 가능하다.
* 각 시점에서 입력 벡터와 이전 활성화 값이 결합되어 새로운 활성화 값이 계산된다.
* 순전파 과정에서 새로운 활성화 값을 계산할 때, tanh, ReLU 활성화 함수가 활용된다.
* 출력값 예측에서는 이진 분류의 경우 Sigmoid, 다중 분류의 경우 Softmax를 사용한다.
* RNN 한계: 해당 RNN은 이전 시점의 정보만을 활용하기 때문에 Sequence 뒤쪽에 있는 정보를 사용하지 못하는 한계가 있다. -> 추후 이를 개선한 양방향 RNN인 BRNN 등장

### 1.2 Backpropagation through time
* 순전파(Forward-propagation): 입력 sequence를 사용하여 각 단계에서 은닉 상태를 계산하고 이를 통해 예측값을 출력하는 형태를 의미한다.
* 역전파(Back-propagation): 순전파의 반대 방향(오른쪽 -> 왼쪽)으로 계산하는 것으로 매개변수에 대한 기울기를 계산해 경사하강법으로 모델의 매개변수를 업데이트한다.      
![photo 120](/assets/img/blog/img120.png)       
* 손실 계산: 각 단계에서 예측값과 실제값의 차이를 기반으로 손실을 계산하고 전체 sequence의 손실은 모든 단계의 손실합으로 정의한다.

### 1.3 Different types of RNNs
* input sequence와 output sequence의 길이가 항상 같을 필요는 없다.
* RNN types
  1. One to one: 일반적인 신경망 구조로 입력값과 출력값이 하나인 경우
  2. One to many: 입력값은 하나이고 출력값이 sequence인 경우 / ex) 음악 생성
  3. Many to one: 입력값이 sequence이고 출력값이 하나인 경우 / ex) 감성 분류
  4. Many to many: 입력값과 출력값의 sequence가 동일한 경우(ex. 개체명 인식) or 다른 경우(ex.기계 번역)
     * 기계 번역처럼 입력과 출력 sequence가 다른 경우, RNN은 incoder와 decoder로 구성되어 입력 sequence를 처리한다.          
![photo 121](/assets/img/blog/img121.png)        
 
### 1.4 Language model and sequence generation
* language model은 단어 sequence를 입력으로 받아 그 sequence의 확률을 추정하기 위해 corpus를 사용해 학습한다. (corpus: 대규모 텍스트 데이터)
* language model은 문장을 단어(token) 단위로 나누고(tokenizer), 각 단어를 one-hot vector or index로 변환한다. 또한, 문장을 끝을 나타내기 위해 EOS token을 추가하거나 어휘에 없는 단어는 UNK(unknown token)로 처리한다.       
![photo 122](/assets/img/blog/img122.png)                
* RNN은 softmax loss function을 기반으로 학습하여 각 단계의 예측과 실제값 간의 차이를 최소화한다.

### 1.5 Sampling novel sequences
* Sampling을 통해 문장을 생성하는 방법:
  1. RNN language model은 학습 이후 소프트맥스 기반으로 단어를 샘플링하여 문장을 생성한다.
  2. 첫 번째 단어를 샘플링한 후, 이를 입력값으로 사용해 다음 단어를 예측한다. (반복을 통한 문장 생성)
  3. 문장이 끝날 때 EOS token을 생성하거나 미리 정해진 단어 수에 도달할 때까지 샘플링을 반복한다.
* 개별 문자를 예측하는 문자 수준의 RNN language model을 만들 수도 있다. 하지만, 더 긴 sequence를 처리해야 하고 계산 비용이 더 크다.
* 문자 수준의 언어 모델은 단어 수준의 언어 모델보다 긴 종속성을 처리하는 데 어려움이 있지만, 더 유연하게 작동한다. 그럼에도 대부분 단어 수준의 언어 모델을 사용한다.
  

### 1.6 Vanishing gradients with RNNs
* RNN은 이전 입력이 이후 출력에 영향을 미치는 장기적인 의존성을 잘 학습하지 못한다. (기억력 낮음)
* Vanishing gradients problem: 매우 긴 sequence를 처리할 때, 역전파 과정에서 값이 소멸하여 초기 입력의 정보가 후반부까지 전달되지 못하는 문제를 의미한다. 이로 인해 장기적인 패턴 학습이 어려워 진다. 
* Exploding gradient problem: 기울기가 기하급수적으로 커지면서 NaN 값을 발생시키는 문제로 gradient clipping을 통해 어느정도 해결 가능하다.        
![photo 123](/assets/img/blog/img123.png)               

### 1.7 Gated Recurrent Unit (GRU)
* GRU는 vanishing gradients problem을 해결하기 위해 나온 개념으로 장거리 의존성을 학습할 수 있도록 도와준다.                 
![photo 124](/assets/img/blog/img124.png)                     
* GRU 작동 원리
  1. 메모리 셀(c) 도입을 통해 중요한 정보를 기억하도록 한다.
  2. 게이트(Gamma_u) 도입을 통해 정보를 업데이트할 시점을 결정한다.
  3. GRU는 각 단계에서 후보 메모리 셀(c)을 계산한다.
  4. 게이트(Gamma_u)는 정보 업데이트 유무를 결정한다. 1이면 메모리 셀을 업데이트하고 0이면 기존 메모리 셀을 유지한다.
* Full GRU: 기존 GRU에 적절성 게이트(Gamma_r)를 추가한 것으로 이전 단계의 정보 반영 양을 결정한다.

### 1.8 LSTM (long short term memory) unit
* LSTM: GRU보다 더 복잡한 구조를 가진 RNN unit으로 vanishing gradients problem을 해결하고 장기적인 의존성을 잘 학습할 수 있다.
* LSTM은 GRU와 유사하게 메모리 셀(c_t)을 사용하지만, 더 많은 게이트를 사용하여 정보를 유연하게 처리한다.
* LSTM의 3가지 gate
  1. Update gate(Gamma_u): 새로운 정보 업데이트 유무 결정
  2. Forget gate(Gamma_f): 이전 메모리 셀 값을 유지량 결정
  3. Output gate: 메모리 셀의 출력 결정                
![photo 125](/assets/img/blog/img125.png)        
* LSTM 작동 방식: 메모리 셀(c_t)은 forget gate와 update gate에 의해 갱신되고 출력은 output gate에 의해 결정된다.
* GRU VS LSTM
  * GRU: 2개의 gate를 사용하고 단순하며 계산이 빠르다.
  * LSTM: GRU에 비해 복잡하지만 더 좋은 성능을 제공한다. 

### 1.9 Bidirectional RNN
* Bidirectional RNN(BRNN): 양방향 RNN으로 일반적인 정보 처리(왼쪽 -> 오른쪽)와 다르게 sequence의 양쪽 정보를 모두 활용하여 정보를 처리한다. 이에 더 정확한 예측이 가능하고 문장 내 단어의 의미를 잘 파악할 수 있다.           
![photo 126](/assets/img/blog/img126.png)        
* BRNN 작동 원리
  1. 두 개의 RNN Network(forward, back)를 사용하여 입력 sequence를 처리한다.
  2. 순방향 네트워크는 왼쪽에서 오른쪽으로, 역방향 네트워크는 오른쪽에서 왼족으로 정보를 처리한다.
  3. 순방향 네트워크와 역방향 네트워크의 정보를 결합해 예측을 생성한다.
* BRNN은 기존 RNN, GRU, LSTM과 함께 사용될 수 있다.
* BRNN은 전체 sequence가 필요하므로 실시간 처리에는 적합하지 않다.

### 1.10 Deep RNNs
* Deep RNN: RNN의 여러 계층을 쌓아 더 복잡한 sequence를 학습하고 처리위한 모델이다. 각 층은 이전 계층의 출력을 입력으로 받아 처리한다.
* 각 계층의 활성화 값은 이전 계층의 활성화 값과 함께 사용되며, 같은 매개변수 세트를 공유한다.     
![photo 127](/assets/img/blog/img127.png)        
* Deep RNN은 standard RNN, GRU, LSTM 기반으로 구성할 수 있으며 BRNN을 심층으로 만들 수 있다.
* Deep RNN은 훈련 비용이 많이 들지만 복잡한 패턴을 학습할 수 있다는 측면에서 유용하다.    

## 2. NLP & Word Embeddings

### 2.1 Word Representation
* 기존 one-hot vector는 단어 간의 유사성을 반영하지 못하고 독립적으로 보았다.
* 이를 해결하기 위해 단어를 고차원 vector로 표현하는 embedding을 통해 유사한 단어들을 가까운 벡터로 표현할 수 있다.
* word embedding을 통해 단어 간의 유사성을 파악하고 모델이 교차 단어를 더 잘 일반화할 수있다.
* embedding vector는 고차원 vector로 표현되므로 이를 t-SNE와 같은 알고리즘을 통해 2차원으로 시각화할 수 있다. 이를 통해 유사한 단어들이 가까운 위치에 나타나는 것을 알 수 있다.
![photo 128](/assets/img/blog/img128.png)         
 
### 2.2 Using Word Embeddings
* word embedding을 통해 단어를 고차원 vector로 표현하면, 학습 알고리즘이 단어 간의 유사성을 더 잘 이해할 수 있다.
* word embedding을 전이 학습에도 활용할 수 있다. 대규모 비라벨 텍스트 corpus에서 word embedding을 학습한 후, 이를 작은 레이블이 있는 training set에 적용하면 다양한 NLP 작업을 수행할 수 있다.
* word embedding은 named entity recognition, 구문 분석(parsing), 텍스트 요약 등의 유용하게 활용할 수 있다.

### 2.3 Properties of Word Embeddings
* word embedding을 통해 단어 간의 관계를 벡터 연산을 통해 수치적으로 나타낼 수 있다.
* cosine similarity: 벡터 간의 관계를 측정하는 대표적인 방법으로 두 벡터 간의 cos값을 계산하여 유사도를 측정한다.         
![photo 129](/assets/img/blog/img129.png)        
![photo 130](/assets/img/blog/img130.png)              

### 2.4 Embedding Matrix
* word embedding은 one-hot vector와 embedding matrix 곱을 통해 고차원 공간 저차원 벡터로 변환한다.              
![photo 131](/assets/img/blog/img131.png)          
 
### 2.5 Word2Vec
* Word2Vec: word embedding을 학습하는 방법으로 단어를 벡터로 표현하여 그 벡터가 단어의 의미적 관계를 반영하도록 하는 기술이다.
* Skip-Gram model: Word2Vec의 핵심 구성 요소 중 하나로 주어진 문장의 특정 단어가 주어지면 그 주변에 있는 단어(target word)를 예측한다.          
![photo 132](/assets/img/blog/img132.png)            
* Skip-Gram model 예측 방식: word window 안에서 주어진 특정 단어를 기준으로 target word를 선택하여 예측한다.
* Skip-Gram model 학습 과정
  1. 단어들을 one hot vector로 표현한 뒤, 해당 벡터를 embedding matrix에 통과시켜 embedding vector를 얻는다.
  2. softmax 함수를 사용해 주어진 특정 단어를 기준으로 각 단어가 target word일 확률을 계산한다.
  3. cross entropy loss 최소화를 통해 실제 target word와 예측된 word 분포 간의 차이를 줄인다.
* Skip-Gram model 단점:
  1. softmax 함수로 모든 단어의 확률을 계산하므로 대규모 vocabulary를 사용할 때는 계산 비용이 매우 크다.
* Skip-Gram model 단점 해결 방안
  1. Hierarchical Softmax: 단어들을 계층 구조로 분류하여 필요한 계산량을 줄인다.        
  ![photo 133](/assets/img/blog/img133.png)              
  3. Negative Sampling을 통해 간단하고 효율적인 계산이 가능하다.
  
### 2.6 Negative Sampling
* Negative Sampling은 Word2Vec의 Skip-Gram 모델을 더 효율적으로 학습하기 위한 방법이다.
* Negative Sampling 학습 방법: 각 긍정적인 예제와 함께 여러 개의 부정적인 예제를 생성하여 학습한다. 부정적인 예제를 많이 포함시켜 softmax 대신 이진 분류 문제로 변환하면서 계산 비용을 크게 줄일 수 있다.      
![photo 134](/assets/img/blog/img134.png)              
* 부정적인 예제는 dataset에서 빈도가 낮은 단어를 주로 사용하며 경험적 빈도 기반으로 샘플링한다.
* Negative Sampling을 통해 계산 효율성을 높이면서 좋은 품질의 단어 vector를 얻을 수 있다.

### 2.7 Glove word Vectors
* Glove Algorithms: 텍스트 내에서 각 단어가 다른 단어들과 얼마나 자주 함께 나타내는지를 고려하여 단어 간의 관계를 학습한다.
* Glove Algorithms은 단어 i가 문맥에서 단어 j와 얼마나 자주 등장하는지를 나타내는 값인 X_ij를 사용하여 두 단어 사이의 관계를 모델링한다.
* Glove Algorithms은 아래 식을 최소화하는 것을 목표로 하여 단어 쌍의 출현 빈도를 바탕으로 두 단어가 관련성을 예측하는 벡터를 학습한다.     
  ![photo 135](/assets/img/blog/img135.png)     
  * f(X_ij)는 자주 등장하는 단어들에 과도한 영향을 주지 않기 위한 가중치 함수다. 해당 함수는 출현 빈도가 높은 단어에 대해서는 가중치를 낮추고 반대로 낮은 단어에 대해서는 더 높은 가중치를 부여한다.
  * theta와 e는 대칭적인 역할을 하고, 경사하강법을 통해 해당 벡터들을 최적화한다.
* Glove Algorithms 장점
  1. 단어 간의 유사성을 학습하는 데 효과적이다.
  2. 평행사변형 관계를 기반으로 유추 문제를 잘 해결할 수 있다.

### 2.8 Sentiment Classification
* Sentiment Classification: 감성 분류는 텍스트의 감정을 긍정적 또는 부정적으로 예측하는 작업이다.
![photo 136](/assets/img/blog/img136.png)  
* word embedding을 사용한 간단한 sentiment classification model은 텍스트 내 단어를 embedding vector로 변환한 후, 이 vector들의 합계 or 평균을 계산하여 sofmax classifier로 감정을 예측한다.
* 위의 방법은 텍스트 길이에 상관없이 동작하지만, 단어 순서를 무시한다는 단점이 있다.
* RNN을 통해 문장의 단어 순서를 분석하여 더 정확한 감성 분류 모델을 만들 수 있다.
* 또한, word embedding을 통해 Sentiment classifier를 작은 training set로도 효과적으로 만들 수 있다.

### 2.9 Debiasing word Embeddings
* word embedding의 포함된 bias을 줄이는 방법
  1. 성별과 같은 특정 bias 방향을 SVD(특이값 분해) 통해 식별한다.           
    ![photo 137](/assets/img/blog/img137.png)                     
  2. 중립화 단계를 통해 성중립적 단어에서 bias 방향에서의 성별 요소 제거한다.         
    ![photo 138](/assets/img/blog/img138.png)             
  3. 평준화 단계를 통해 유사한 단어 쌍들이 성중립적인 단어와 같은 거리나 유사성을 유지하도록 만든다.              
    ![photo 139](/assets/img/blog/img139.png)            
* Debiasing word Embeddings을 통해 AI 시스템이 성별이나 인종 등에 대한 부적절한 bias를 반영하지 않도록 할 수 있다.
<br>       

참고문헌: C5W1 & C5W2

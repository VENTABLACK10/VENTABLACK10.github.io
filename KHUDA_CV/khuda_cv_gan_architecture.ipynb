{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338a728f-47a9-45e9-bdaf-c1dacc93877f",
   "metadata": {},
   "source": [
    "### <b>1. ProGAN<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb879927-5c13-44ff-8ae5-4b584bbb7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# PixelNorm Layer\n",
    "# 채널별로 정규화 -> 학습 안정화\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, eps=1e-8):\n",
    "        # 각 pixel vector의 L2-norm으로 정규화\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + eps)\n",
    "\n",
    "# Equalized Learning Rate 적용 Conv2d\n",
    "# 학습 안정화를 위한 스케일링 적용\n",
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding)\n",
    "        # 논문: He 초기화 후 weight를 정규분포로 초기화\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        # 스케일링 계수 (He initialization과 동일한 분산 사용)\n",
    "        self.scale = (2 / (in_ch * kernel_size ** 2)) ** 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale)\n",
    "\n",
    "# Equalized Learning Rate 적용 Linear\n",
    "# FC layer version\n",
    "class WSLinear(nn.Module):\n",
    "    def __init__(self, in_f, out_f):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_f, out_f)\n",
    "        nn.init.normal_(self.linear.weight)\n",
    "        # He 초기화 기반 스케일링\n",
    "        self.scale = (2 / in_f) ** 0.5 \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x * self.scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b9cd29-4ebd-4703-a50c-18f18a8422fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "# Block 순서 \n",
    "# Conv → LeakyReLU → PixelNorm → Conv → LeakyReLU → PixelNorm\n",
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            WSConv2d(in_ch, out_ch, 3, 1, 1), # Equalized LR 적용\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(), # Pixel 단위 정규화\n",
    "            WSConv2d(out_ch, out_ch, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, base_channels=512):\n",
    "        super().__init__()\n",
    "        # 초기 fully-connected + reshape 단계:\n",
    "        # z → 4x4 feature map\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(), # latent 벡터 정규화\n",
    "            WSLinear(z_dim, base_channels * 4 * 4), # z → 4x4 feature map으로 변환\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.initial_conv = GenBlock(base_channels, base_channels) # 4x4 block\n",
    "        self.to_rgb_layers = nn.ModuleList([WSConv2d(base_channels, 3, 1, 1, 0)]) # toRGB conv for 4x4\n",
    "        self.blocks = nn.ModuleList()  # 해상도 증가를 위한 Blocks\n",
    "        self.channel_map = [base_channels, 256, 128, 64, 32, 16]  # 해상도별 채널 설정\n",
    "\n",
    "        for i in range(len(self.channel_map) - 1):\n",
    "            in_ch, out_ch = self.channel_map[i], self.channel_map[i + 1]\n",
    "            self.blocks.append(GenBlock(in_ch, out_ch)) # 새로운 Block 추가\n",
    "            self.to_rgb_layers.append(WSConv2d(out_ch, 3, 1, 1, 0)) # toRGB conv\n",
    "\n",
    "    def fade_in(self, alpha, old, new):\n",
    "        # fade-in: 기존 출력을 조금씩 섞으면서 새로운 해상도에 적응\n",
    "        return torch.tanh(alpha * new + (1 - alpha) * old)\n",
    "\n",
    "    def forward(self, z, step, alpha):\n",
    "        out = self.initial(z).view(z.shape[0], self.channel_map[0], 4, 4) # z를 4x4 feature로 변환\n",
    "        out = self.initial_conv(out)\n",
    "\n",
    "        if step == 0:\n",
    "            return torch.tanh(self.to_rgb_layers[0](out)) # 초기 단계는 바로 toRGB\n",
    "\n",
    "        for i in range(step): # step 수만큼 업샘플링과 블록 통과\n",
    "            out = F.interpolate(out, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            out = self.blocks[i](out)\n",
    "\n",
    "        out_new = self.to_rgb_layers[step](out)  # 새 RGB 출력\n",
    "        out_old = self.to_rgb_layers[step - 1](  # 이전 해상도의 RGB 출력\n",
    "            F.interpolate(out, scale_factor=0.5) # 절반으로 축소 후 적용\n",
    "        )\n",
    "        out_old = F.interpolate(out_old, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return self.fade_in(alpha, out_old, out_new) # 두 출력을 섞어서 return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c0799c-ae6f-40e2-b582-b5060079aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "# Blocks: Conv → LeakyReLU → Conv → LeakyReLU\n",
    "class DiscBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            WSConv2d(in_ch, in_ch, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_ch, out_ch, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, base_channels=512):\n",
    "        super().__init__()\n",
    "        self.channel_map = [base_channels, 256, 128, 64, 32, 16] # 채널 매핑\n",
    "        self.blocks = nn.ModuleList() # 해상도 단계별 블록\n",
    "        self.from_rgb = nn.ModuleList([WSConv2d(3, base_channels, 1, 1, 0)])  # 4x4 입력용 fromRGB\n",
    "\n",
    "        for i in range(len(self.channel_map) - 1):\n",
    "            in_ch, out_ch = self.channel_map[i + 1], self.channel_map[i]\n",
    "            self.blocks.append(DiscBlock(in_ch, out_ch)) # 블록 추가\n",
    "            self.from_rgb.append(WSConv2d(3, in_ch, 1, 1, 0)) # fromRGB 추가\n",
    "\n",
    "        self.final_block = nn.Sequential(\n",
    "            nn.AvgPool2d(4), # Global average pooling\n",
    "            WSConv2d(base_channels + 1, base_channels, 3, 1, 1), # Minibatch stddev 포함 처리\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(base_channels, base_channels, 4, 1, 0), # 4x4 → 1x1\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSLinear(base_channels, 1) # 최종 score\n",
    "        ) \n",
    "\n",
    "    def fade_in(self, alpha, old, new):\n",
    "        return alpha * new + (1 - alpha) * old # fade-in 적용\n",
    "\n",
    "    def forward(self, x, step, alpha):\n",
    "        if step == 0:\n",
    "            x = self.from_rgb[0](x) # 4x4용 fromRGB\n",
    "            return self.final_block(x)\n",
    "\n",
    "        downscaled = F.avg_pool2d(x, 2) # 이전 단계 입력 생성\n",
    "        out_old = self.from_rgb[step - 1](downscaled) # 이전 단계로 변환\n",
    "        out_new = self.from_rgb[step](x) # 현재 단계로 변환\n",
    "        out_new = self.blocks[step - 1](out_new) # 블록 통과\n",
    "\n",
    "        x = self.fade_in(alpha, out_old, out_new)  # 두 출력을 fade-in\n",
    "\n",
    "        for i in reversed(range(step - 1)):\n",
    "            x = self.blocks[i](x) # 나머지 블록 통과\n",
    "            x = F.avg_pool2d(x, 2) # 다운샘플링\n",
    "\n",
    "        # Minibatch StdDev Trick\n",
    "        batch_std = torch.std(x, dim=0, keepdim=True).mean() # 배치 단위 표준편차\n",
    "        batch_std = batch_std.expand(x.size(0), 1, 4, 4) # 차원 맞추기\n",
    "        x = torch.cat([x, batch_std], dim=1) # 채널에 붙이기\n",
    "\n",
    "        return self.final_block(x) # 최종 판별 결과\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a1eca-0a8d-4001-9016-98db6b7c56ee",
   "metadata": {},
   "source": [
    "### <b>2. StyleGAN<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5a17c1-7858-4403-a7db-a6c23f436497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Network : Latent vector z -> ntermediate latent space w로 변환(8-layer MLP)\n",
    "# Adaptive Instance Normalization (AdaIN): 스타일 벡터 w를 각 레이어에 주입하여 스타일 조절\n",
    "# Constant Input: z를 직접 convolution 하지 않고, 고정된 learnable constant에서 시작\n",
    "# Noise Injection: 각 레이어에 독립적인 노이즈를 더해 세부 묘사 제어\n",
    "# Progressive Growing: ProGAN처럼 해상도를 점진적으로 키움 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da691fa0-e73f-4cdc-835d-5f5e759418cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StyleGAN 기본 구조\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. PixelNorm\n",
    "class PixelNorm(nn.Module):\n",
    "    def forward(self, x, eps=1e-8):\n",
    "        # 각 픽셀 벡터의 L2 norm으로 정규화 -> 학습 안정화\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + eps)\n",
    "\n",
    "# 2. Mapping Network (z → w)\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim, num_layers=8):\n",
    "        super().__init__()\n",
    "        layers = [PixelNorm()] # z 입력 정규화\n",
    "        for _ in range(num_layers): # 8개의 fully connected 레이어\n",
    "            layers.append(nn.Linear(z_dim, w_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            z_dim = w_dim\n",
    "        self.mapping = nn.Sequential(*layers) # 전체 mapping 네트워크\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.mapping(z) # z → w 로 변환\n",
    "\n",
    "# 3. Noise Injection\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # 노이즈를 각 채널에 적용할 때 곱해줄 learnable weight\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "\n",
    "    def forward(self, x, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x) # 없으면 랜덤 노이즈 생성\n",
    "        return x + self.weight * noise # 노이즈 주입\n",
    "\n",
    "# 4. AdaIN (w 벡터로 style 조절)\n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Linear(w_dim, channels) # w → scale\n",
    "        self.bias = nn.Linear(w_dim, channels) # w → bias\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        mean = x.mean(dim=(2, 3), keepdim=True) # feature map 평균\n",
    "        std = x.std(dim=(2, 3), keepdim=True) # feature map 표준편차\n",
    "        normalized = (x - mean) / (std + 1e-8) # 정규화\n",
    "        style_scale = self.scale(w).unsqueeze(2).unsqueeze(3) # w로부터 scale\n",
    "        style_bias = self.bias(w).unsqueeze(2).unsqueeze(3) # w로부터 bias\n",
    "        return style_scale * normalized + style_bias # 스타일 변환 결과\n",
    "\n",
    "# 5. StyledConv (Noise + AdaIN + Conv)\n",
    "class StyledConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, w_dim, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, padding=padding)\n",
    "        self.noise = NoiseInjection(out_ch) # 노이즈 주입 모듈\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.adain = AdaIN(out_ch, w_dim) # AdaIN 모듈\n",
    "\n",
    "    def forward(self, x, w, noise=None):\n",
    "        x = self.conv(x)\n",
    "        x = self.noise(x, noise) # 노이즈 추가\n",
    "        x = self.activation(x)\n",
    "        x = self.adain(x, w) # AdaIN을 통해 스타일 적용\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6c84ae-0e71-4741-beca-8a377199ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512, channels=[512, 256, 128, 64, 32]):\n",
    "        super().__init__() \n",
    "        self.mapping = MappingNetwork(z_dim, w_dim) # Mapping 네트워크\n",
    "        self.constant_input = nn.Parameter(torch.randn(1, channels[0], 4, 4)) # learnable constant 시작점\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.to_rgb = nn.ModuleList() # 각 해상도에서 RGB로 변환하는 레이어\n",
    " \n",
    "        in_ch = channels[0]\n",
    "        for out_ch in channels:\n",
    "            # StyledConv 2개가 쌍으로 있는 block 추가\n",
    "            self.blocks.append(nn.ModuleList([\n",
    "                StyledConv(in_ch, out_ch, w_dim),\n",
    "                StyledConv(out_ch, out_ch, w_dim)\n",
    "            ]))\n",
    "            self.to_rgb.append(nn.Conv2d(out_ch, 3, 1)) # toRGB conv\n",
    "            in_ch = out_ch\n",
    "\n",
    "    def forward(self, z, step=0, alpha=1.0, noise=None):\n",
    "        w = self.mapping(z)  # z → w\n",
    "        x = self.constant_input.expand(z.shape[0], -1, 4, 4) # constant 입력 복사\n",
    "\n",
    "        for i in range(step + 1): # 현재 해상도까지 반복\n",
    "            block = self.blocks[i]\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False) if i > 0 else x\n",
    "            x = block[0](x, w) # styled conv 1\n",
    "            x = block[1](x, w) # styled conv 2\n",
    "\n",
    "        img = self.to_rgb[step](x) # RGB 변환\n",
    "        return torch.tanh(img) # 출력 [-1, 1]로 정규화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f13ce2-4c64-4f43-a6bd-26b7cca66882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator (mirror of generator)\n",
    "\n",
    "# Minibatch Stddev layer\n",
    "class MinibatchStdDev(nn.Module):\n",
    "    def forward(self, x):\n",
    "        std = x.std(0, keepdim=True).mean() # 배치의 표준편차 평균\n",
    "        shape = list(x.shape)\n",
    "        shape[1] = 1\n",
    "        std_map = std.expand(shape) # 모든 위치에 복사\n",
    "        return torch.cat([x, std_map], 1)\n",
    "\n",
    "class DiscBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, in_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        return F.avg_pool2d(x, 2) # 다운샘플링\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=[512, 256, 128, 64, 32]):\n",
    "        super().__init__()\n",
    "        self.from_rgb = nn.ModuleList() # 이미지 → feature map\n",
    "        self.blocks = nn.ModuleList() \n",
    "        in_ch = channels[-1]\n",
    "\n",
    "        for out_ch in reversed(channels[:-1]):\n",
    "            self.from_rgb.append(nn.Conv2d(3, in_ch, 1)) # RGB 변환\n",
    "            self.blocks.append(DiscBlock(in_ch, out_ch))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.stddev = MinibatchStdDev() # Minibatch stddev\n",
    "        self.final_conv = nn.Conv2d(channels[0]+1, channels[0], 3, padding=1)\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels[0]*4*4, 1) # score 출력\n",
    "        )\n",
    "\n",
    "    def forward(self, img, step=0, alpha=1.0):\n",
    "        x = self.from_rgb[step](img) # RGB → feature\n",
    "        for i in range(step, -1, -1): # 거꾸로 처리\n",
    "            x = self.blocks[i](x)\n",
    "        x = self.stddev(x) # Minibatch stddev 추가\n",
    "        x = self.final_conv(x)\n",
    "        return self.final_fc(x) # score 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb45b0a-7dcb-4534-9ef3-83bd585e7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN-GP Loss\n",
    "# gradient penalty\n",
    "def gradient_penalty(d, real, fake, step, alpha):\n",
    "    batch_size = real.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1, device=real.device) # 랜덤 보간 계수\n",
    "    interpolated = real * epsilon + fake * (1 - epsilon) # 중간값\n",
    "    interpolated.requires_grad_() \n",
    "\n",
    "    d_interpolated = d(interpolated, step, alpha) # 판별\n",
    "    grad = torch.autograd.grad(\n",
    "        outputs=d_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(d_interpolated),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    grad = grad.view(grad.size(0), -1) # 2D\n",
    "    gp = ((grad.norm(2, dim=1) - 1) ** 2).mean()  # L2-norm 기반 penalty\n",
    "    return gp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a4f3c0-d5f3-4f5d-b4a6-9a53f5941585",
   "metadata": {},
   "source": [
    "### <b>3. StyleGAN2<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9954317-6cc5-49f0-9553-139098d5aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선사항\n",
    "# Weight Demodulation: AdaIN 대신 weight demodulation을 통해 스타일 적용\n",
    "# Blurred Upsample / Downsample: aliasing 제거를 위해 blur kernel 사용\n",
    "# ResNet-style Skip Connection: Discriminator에 skip 연결 도입\n",
    "# Progressive Growing 제거: 고정된 해상도에서 바로 학습 가능 (fused training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2f40e02-1c37-4d26-83c4-fcd8895f3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# PixelNorm\n",
    "class PixelNorm(nn.Module):\n",
    "    def forward(self, x, eps=1e-8):\n",
    "        # 채널 단위 평균 제곱값으로 나눠 정규화 -> 학습 안정화\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + eps)\n",
    "\n",
    "# Mapping Network: z → w\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512, num_layers=8):\n",
    "        super().__init__()\n",
    "        layers = [PixelNorm()] # PixelNorm을 통해 z 정규화\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(z_dim, w_dim)) # FC layer로 latent 변환\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        self.mapping = nn.Sequential(*layers) # Sequential로 묶기\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.mapping(z) # 입력 z → 스타일 벡터 w\n",
    "\n",
    "# Noise Injection (per-channel learnable scale)\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # 각 채널마다 노이즈의 중요도를 조절하는 learnable weight\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "\n",
    "    def forward(self, x, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x) # 랜덤 노이즈 생성\n",
    "        return x + self.weight * noise # 노이즈 주입\n",
    "\n",
    "# ModulatedConv2d: 핵심 모듈 (StyleGAN2 핵심)\n",
    "class ModulatedConv2d(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, style_dim, kernel_size=3, demodulate=True):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-8\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.kernel_size = kernel_size\n",
    "        self.demodulate = demodulate\n",
    "\n",
    "        # Conv weight: (1, out_ch, in_ch, k, k) → 스타일마다 스케일링\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_ch, in_ch, kernel_size, kernel_size)\n",
    "        )\n",
    "        # 스타일 벡터를 입력 채널 차원에 맞게 조정하는 FC layer\n",
    "        self.style = nn.Linear(style_dim, in_ch)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        batch, _, height, width = x.shape\n",
    "        \n",
    "        # 스타일 벡터를 input 채널에 맞게 변환 (scale 역할)\n",
    "        style = self.style(w).view(batch, 1, self.in_ch, 1, 1)\n",
    "        \n",
    "        # weight modulation: 스타일로 스케일링\n",
    "        weight = self.weight * style\n",
    "\n",
    "        # weight demodulation: 각 out 채널의 분산을 1로 정규화\n",
    "        if self.demodulate:\n",
    "            demod = torch.rsqrt((weight ** 2).sum([2, 3, 4]) + self.eps)\n",
    "            weight = weight * demod.view(batch, self.out_ch, 1, 1, 1)\n",
    "\n",
    "        # convolution을 위해 weight와 x를 reshape\n",
    "        weight = weight.view(batch * self.out_ch, self.in_ch, self.kernel_size, self.kernel_size)\n",
    "        x = x.view(1, batch * self.in_ch, height, width)\n",
    "        \n",
    "        # Grouped convolution을 통해 스타일별 독립 처리\n",
    "        out = F.conv2d(x, weight, padding=self.kernel_size // 2, groups=batch)\n",
    "        return out.view(batch, self.out_ch, height, width) # 원래 배치 형태로 복원\n",
    "\n",
    "# StyledConvBlock: ModConv + Noise + Activation\n",
    "class StyledConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, style_dim):\n",
    "        super().__init__()\n",
    "        # 스타일 기반 modulated conv\n",
    "        self.modconv = ModulatedConv2d(in_ch, out_ch, style_dim)\n",
    "        self.noise = NoiseInjection(out_ch) # 채널별 노이즈 주입\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, w, noise=None):\n",
    "        x = self.modconv(x, w) # 스타일에 따라 weight modulation된 conv\n",
    "        x = self.noise(x, noise) # 노이즈 추가\n",
    "        return self.activation(x)\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512, channels=[512, 256, 128, 64, 32]):\n",
    "        super().__init__()\n",
    "        self.mapping = MappingNetwork(z_dim, w_dim) # Mapping 네트워크\n",
    "        # 처음에 사용할 learnable constant (4x4 feature map)\n",
    "        self.constant = nn.Parameter(torch.randn(1, channels[0], 4, 4))\n",
    "\n",
    "        self.blocks = nn.ModuleList() # 스타일 블록 리스트\n",
    "        self.to_rgb = nn.ModuleList() # RGB로 변환하는 conv 리스트\n",
    "\n",
    "        in_ch = channels[0]\n",
    "        for out_ch in channels:\n",
    "            # 각 해상도별 conv2개 블록 생성\n",
    "            self.blocks.append(nn.ModuleList([\n",
    "                StyledConvBlock(in_ch, out_ch, w_dim),\n",
    "                StyledConvBlock(out_ch, out_ch, w_dim)\n",
    "            ]))\n",
    "            self.to_rgb.append(nn.Conv2d(out_ch, 3, kernel_size=1)) # toRGB layer\n",
    "            in_ch = out_ch\n",
    "\n",
    "    def forward(self, z, noise=None):\n",
    "        w = self.mapping(z) # z → w 스타일 벡터 생성\n",
    "        x = self.constant.expand(z.shape[0], -1, 4, 4) # 배치 크기에 맞게 constant input 복제\n",
    "\n",
    "        # 해상도 단계별로 업샘플 → 블록 통과\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False) if i > 0 else x\n",
    "            x = block[0](x, w, noise)\n",
    "            x = block[1](x, w, noise)\n",
    "            \n",
    "        # 최종 RGB 이미지 출력 (3채널)\n",
    "        img = self.to_rgb[-1](x)\n",
    "        return torch.tanh(img) # [-1, 1] 범위로 정규화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5af516a4-1f3f-4ea1-8e24-6c3f07c587a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, in_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Skip connection (Residual 경로)\n",
    "        # x를 1x1 Conv로 변환하고, 다운샘플링 (Average Pooling)\n",
    "        residual = F.avg_pool2d(self.skip(x), 2) \n",
    "        x = self.activation(self.conv1(x)) # 메인 경로 - 첫 번째 Conv 후 LeakyReLU\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 2) # 해상도를 줄이기 위해 평균 풀링 적용 (2x down)\n",
    "        # Residual 연결 + 정규화 (sqrt(2)로 나누는 건 ResNet의 variance-preserving trick)\n",
    "        return (x + residual) / math.sqrt(2)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=[512, 256, 128, 64, 32]):\n",
    "        super().__init__()\n",
    "        # 입력 이미지를 feature map으로 변환하는 1x1 Conv\n",
    "        self.from_rgb = nn.Conv2d(3, channels[-1], 1) # in_ch → out_ch 블록 추가\n",
    "        self.blocks = nn.ModuleList() # ResBlock 리스트\n",
    "\n",
    "        in_ch = channels[-1]\n",
    "        # 채널을 거꾸로 줄여가며 ResBlock 구성\n",
    "        for out_ch in reversed(channels[:-1]):\n",
    "            self.blocks.append(DiscResBlock(in_ch, out_ch))\n",
    "            in_ch = out_ch # 다음 블록 입력 채널로 설정\n",
    "\n",
    "        # 마지막 단계: Conv → Flatten → Linear로 score 예측\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, in_ch, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_ch * 4 * 4, 1) # 4x4 feature map → scalar 판별값\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.from_rgb(x) # RGB 이미지 → feature map (3채널 → channels[-1]\n",
    "        for block in self.blocks:  # 각 블록을 순서대로 통과\n",
    "            x = block(x)\n",
    "        return self.final(x) # 최종 classifier로 score 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024d90f-5aa9-41d2-b075-a42e2453f5c0",
   "metadata": {},
   "source": [
    "### <b>4. SAGAN<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea397b94-ead8-4549-8c7f-fdc464898682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention Layer: CNN의 지역적인 receptive field 한계를 보완하기 위해 전역적인 정보 상호작용 추가\n",
    "# Spectral Normalization: Discriminator에 적용 → 학습 안정성 향상\n",
    "# Residual Block: 안정적인 학습을 위한 ResNet 구조 활용\n",
    "# BatchNorm 대신 Conditional BatchNorm: Class-aware 이미지 생성에 적합 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8337c73-a828-46dc-85f7-76d9066e03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Self-Attention Layer\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # Query, Key: in_channels → in_channels / 8 (차원 축소)\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)  # Value: 원래 채널 크기 유지\n",
    "         # 학습 가능한 스칼라: attention 출력 비율 조절\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))  # 학습 가능한 scale 계수\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape # 배치, 채널, 세로, 가로\n",
    "\n",
    "        # Query, Key, Value 계산\n",
    "        proj_query = self.query_conv(x).view(B, -1, H * W).permute(0, 2, 1)  # [B, HW, C//8]\n",
    "        proj_key = self.key_conv(x).view(B, -1, H * W)                       # [B, C//8, HW]\n",
    "        # Query × Keyᵀ = attention 에너지 (유사도)\n",
    "        energy = torch.bmm(proj_query, proj_key)                            # [B, HW, HW]\n",
    "        # softmax로 attention map 생성 (행 기준 정규화)\n",
    "        attention = F.softmax(energy, dim=-1)  # attention map\n",
    "\n",
    "        proj_value = self.value_conv(x).view(B, -1, H * W)                  # [B, C, HW]\n",
    "        # attention을 value에 곱한다.\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))             # [B, C, HW]\n",
    "        # 다시 spatial map 형태로 reshape\n",
    "        out = out.view(B, C, H, W)\n",
    "\n",
    "        # 입력과 attention 결과를 결합 (잔차 연결 + 학습 가능한 스케일링)\n",
    "        return self.gamma * out + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "482ba350-c8b5-45a3-acaa-f1b6d3067361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator(ex)\n",
    "# Transposed Conv + Attention\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128, channels=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # z: (B, z_dim) → (B, ch*8, 4, 4)\n",
    "            nn.ConvTranspose2d(z_dim, channels * 8, 4, 1, 0),   # 1x1 → 4x4\n",
    "            nn.BatchNorm2d(channels * 8),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 4x4 → 8x8\n",
    "            nn.ConvTranspose2d(channels * 8, channels * 4, 4, 2, 1),  # 4x4 → 8x8\n",
    "            nn.BatchNorm2d(channels * 4),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 8x8 → 16x16\n",
    "            nn.ConvTranspose2d(channels * 4, channels * 2, 4, 2, 1),  # 8x8 → 16x16\n",
    "            nn.BatchNorm2d(channels * 2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 16x16 → 32x32\n",
    "            nn.ConvTranspose2d(channels * 2, channels, 4, 2, 1),  # 16x16 → 32x32\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Self-Attention 삽입: 32x32 해상도에서 전역 상호작용\n",
    "            SelfAttention(channels),  # Attention 위치: 32x32 해상도\n",
    "            nn.ConvTranspose2d(channels, 3, 4, 2, 1),  # 32x32 → 64x64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: (B, z_dim) → (B, z_dim, 1, 1)로 reshape\n",
    "        return self.net(z.view(z.size(0), z.size(1), 1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "234995d8-961b-4f29-965b-c7d9016ca357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Normalized Conv2d\n",
    "def SNConv2d(*args, **kwargs):\n",
    "    return nn.utils.spectral_norm(nn.Conv2d(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4236cad-b3ff-4813-98e9-4956f4d46d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator (SpectralNorm + Self-Attention) \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 입력 이미지 → feature map\n",
    "            SNConv2d(3, channels, 4, 2, 1),       # 64x64 → 32x32\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            SNConv2d(channels, channels * 2, 4, 2, 1),  # 32x32 → 16x16\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            SNConv2d(channels * 2, channels * 4, 4, 2, 1),  # 16x16 → 8x8\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # # Self-Attention 삽입\n",
    "            # 8x8 해상도에서 전역 정보 파악\n",
    "            SelfAttention(channels * 4),  # Attention 위치: 8x8\n",
    "            SNConv2d(channels * 4, channels * 8, 4, 2, 1),  # 8x8 → 4x4\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            SNConv2d(channels * 8, 1, 4)  # 4x4 → 1x1 → 스칼라\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(-1)  # 결과 shape: (B, 1, 1, 1) → (B,)로 펼친다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bc6ee-fa72-42ed-8f5f-a785c7c86117",
   "metadata": {},
   "source": [
    "### <b>5. BigGAN<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ea52aa4-6b33-417f-abe9-2b6895ea9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-conditional GAN:입력에 클래스 정보를 넣어 class-aware 이미지 생성\n",
    "# self-Attention: SAGAN에서 도입한 전역 정보 교환 방식 채택\n",
    "# Spectral Normalization: 안정적 학습을 위해 모든 Conv/Linear에 적용\n",
    "# Shared Embedding: 클래스 임베딩을 여러 레이어에서 공유 (G에서 style modulation처럼 사용)\n",
    "# Orthogonal Regularization: Discriminator의 안정화를 위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c30e5e7c-68e8-45b6-84aa-3f8ed8a6ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConditionalBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features, affine=False) # 일반 BatchNorm, gamma/beta는 제거\n",
    "        # 클래스에 따라 gamma와 beta를 학습하는 embedding\n",
    "        self.gamma_embed = nn.Embedding(num_classes, num_features)\n",
    "        self.beta_embed = nn.Embedding(num_classes, num_features)\n",
    "\n",
    "        # 임베딩 초기화\n",
    "        # 초기화: gamma=1, beta=0 (초기에는 일반 BN처럼 동작)\n",
    "        nn.init.ones_(self.gamma_embed.weight)\n",
    "        nn.init.zeros_(self.beta_embed.weight)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.bn(x) # 일반 BatchNorm 수행\n",
    "        gamma = self.gamma_embed(y).unsqueeze(2).unsqueeze(3)  # [B, C] → [B, C, 1, 1]\n",
    "        beta = self.beta_embed(y).unsqueeze(2).unsqueeze(3)\n",
    "        return gamma * out + beta # 클래스 조건에 따라 gamma/beta 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a269edf-ab07-4d4a-ba08-946189e5dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module): # SAGAN 구조 기반\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)\n",
    "        key = self.key(x).view(B, -1, H * W)\n",
    "        energy = torch.bmm(query, key)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "\n",
    "        value = self.value(x).view(B, -1, H * W)\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1)).view(B, C, H, W)\n",
    "\n",
    "        return self.gamma * out + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37909b19-94cd-4f14-9fc3-bf9d8d4c1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator용 ResBlock (Upsampling 포함)\n",
    "class GenResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, num_classes):\n",
    "        super().__init__()\n",
    "        # 클래스 조건을 받아 BatchNorm에 적용\n",
    "        self.cbn1 = ConditionalBatchNorm(in_ch, num_classes)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2) # 해상도 2배 증가\n",
    " \n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 3, 1, 1))\n",
    "        self.cbn2 = ConditionalBatchNorm(out_ch, num_classes)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_ch, out_ch, 3, 1, 1))\n",
    "        \n",
    "        # 채널 크기가 다르면 skip 연결을 맞춰주는 1x1 conv 추가\n",
    "        self.bypass = nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.relu1(self.cbn1(x, y))\n",
    "        out = self.upsample(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu2(self.cbn2(out, y))\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # skip connection (업샘플 + 채널 정렬)\n",
    "        skip = self.upsample(x)\n",
    "        if self.bypass:\n",
    "            skip = self.bypass(skip)\n",
    "        return out + skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3a8d507-0275-4076-b68b-cdbaffdb26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator용 ResBlock (다운샘플링 포함)\n",
    "class DiscResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, downsample=True):\n",
    "        super().__init__()\n",
    "        # Spectral Normalization이 적용된 Conv2d\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 3, 1, 1))\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_ch, out_ch, 3, 1, 1))\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.downsample = downsample # 다운샘플링 여부\n",
    "\n",
    "        self.bypass = nn.Sequential()\n",
    "        if in_ch != out_ch or downsample:\n",
    "            # 잔차 연결 시 채널/해상도 맞춰주는 1x1 conv\n",
    "            self.bypass = nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 메인 경로: conv → activation → conv\n",
    "        out = self.activation(self.conv1(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            out = F.avg_pool2d(out, 2) # 2x2 average pooling\n",
    "        \n",
    "        # 스킵 경로\n",
    "        skip = self.bypass(x)\n",
    "        if self.downsample:\n",
    "            skip = F.avg_pool2d(skip, 2)\n",
    "        return out + skip # 잔차 연결\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a236e3f1-d75b-4bca-a976-ced665ceb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128, class_dim=1000, ch=64):\n",
    "        super().__init__()\n",
    "        # latent vector z를 4x4 feature map으로 변환 (시작점)\n",
    "        self.linear = nn.utils.spectral_norm(nn.Linear(z_dim, ch * 16 * 4 * 4))\\\n",
    "        \n",
    "        # ResBlock 순차적으로 업샘플링\n",
    "        self.block1 = GenResBlock(ch * 16, ch * 16, class_dim)  # 4x4 → 8x8\n",
    "        self.block2 = GenResBlock(ch * 16, ch * 8, class_dim)   # 8x8 → 16x16\n",
    "        self.block3 = GenResBlock(ch * 8, ch * 4, class_dim)    # 16x16 → 32x32\n",
    "        \n",
    "        # Self-Attention (논문에서 32x32에 적용)\n",
    "        self.attn = SelfAttention(ch * 4)                       # optional: 32x32\n",
    "        self.block4 = GenResBlock(ch * 4, ch * 2, class_dim)    # 32x32 → 64x64\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(ch * 2) # 마지막 정규화\n",
    "        self.relu = nn.ReLU()\n",
    "        self.to_rgb = nn.Conv2d(ch * 2, 3, 3, 1, 1) # RGB 이미지로 변환\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        # z를 4x4 feature map으로 변환\n",
    "        out = self.linear(z).view(z.size(0), -1, 4, 4)\n",
    "\n",
    "        # 클래스 조건 포함하여 블록 통과\n",
    "        out = self.block1(out, y) # 8x8\n",
    "        out = self.block2(out, y) # 16x16\n",
    "        out = self.block3(out, y) # 32x32\n",
    "        out = self.attn(out) # Attention\n",
    "        out = self.block4(out, y) # 64x64\n",
    "        out = self.relu(self.bn(out))\n",
    "        return torch.tanh(self.to_rgb(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17ee4411-7ef0-4af3-97f8-dd9d19a927a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, class_dim=1000, ch=64):\n",
    "        super().__init__()\n",
    "        # ResBlock을 통해 다운샘플링 반복\n",
    "        self.block1 = DiscResBlock(3, ch, downsample=True)       # 64x64 → 32x32\n",
    "        self.block2 = DiscResBlock(ch, ch * 2, downsample=True)  # 32x32 → 16x16\n",
    "        # # Attention 레이어: 16x16 또는 8x8에서 사용 가능\n",
    "        self.attn = SelfAttention(ch * 2)                         # optional\n",
    "        self.block3 = DiscResBlock(ch * 2, ch * 4, downsample=True) # 16x16 → 8x8\n",
    "        self.block4 = DiscResBlock(ch * 4, ch * 8, downsample=True) # 8x8 → 4x4\n",
    "        self.block5 = DiscResBlock(ch * 8, ch * 16, downsample=True) # 4x4 → 2x2\n",
    "\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        # 판별기 출력용 선형 레이어\n",
    "        self.linear = nn.utils.spectral_norm(nn.Linear(ch * 16, 1))\n",
    "        # 클래스 임베딩 벡터와 내부 피처 간 내적을 통해 클래스 조건 반영\n",
    "        self.embed = nn.utils.spectral_norm(nn.Embedding(class_dim, ch * 16))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.block1(x) # 64→32\n",
    "        out = self.block2(out) # 32→16\n",
    "        out = self.attn(out) # attention\n",
    "        out = self.block3(out) # 16→8\n",
    "        out = self.block4(out) # 8→4\n",
    "        out = self.block5(out) # 4→2\n",
    "\n",
    "        out = self.relu(out)\n",
    "        out = out.sum(dim=[2, 3])  # Global sum pooling (2x2 → 벡터)\n",
    "\n",
    "        # 클래스 정보와 임베딩을 내적 (projection discriminator)\n",
    "        out_linear = self.linear(out)\n",
    "        out_embed = (self.embed(y) * out).sum(dim=1, keepdim=True)\n",
    "        return out_linear + out_embed # 최종 score 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d9295-0de5-4add-bc80-73178a61417d",
   "metadata": {},
   "source": [
    "### <b>6. ViT VQ-GAN<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef9d784b-4329-4530-aaff-a8b4a65f7031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE: 이미지를 discrete latent 토큰으로 인코딩/디코딩 (vector quantization 사용)\n",
    "# GAN loss: 디코더 퀄리티 향상을 위한 adversarial loss 사용\n",
    "# ViT: 디코더(또는 discriminator)에 Vision Transformer를 도입\n",
    "# Perceptual loss: LPIPS 등 시각적 유사도 유지 목적의 추가 loss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a01e1619-67da-4414-8fdd-48e95ee4c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Encoder: 이미지를 latent feature로 압축\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, 4, 2, 1),  # 64 → 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 4, 2, 1),   # 32 → 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1)    # 16x16 latent (유지)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x) # 이미지 → latent feature map\n",
    "\n",
    "# Vector Quantizer: vector quantization (codebook lookup)\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_codes=512, code_dim=256):\n",
    "        super().__init__()\n",
    "        # embedding table: [num_codes, code_dim]\n",
    "        self.codebook = nn.Embedding(num_codes, code_dim)\n",
    "        self.codebook.weight.data.uniform_(-1 / num_codes, 1 / num_codes)\n",
    "\n",
    "    def forward(self, z):\n",
    "        B, C, H, W = z.shape\n",
    "        # [B, C, H, W] → [B*H*W, C]: spatial 위치별로 펼침\n",
    "        z_flat = z.permute(0, 2, 3, 1).reshape(-1, C) \n",
    "\n",
    "        # 각 벡터와 코드북 간 L2 거리 계산\n",
    "        distances = (\n",
    "            z_flat.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * z_flat @ self.codebook.weight.t()\n",
    "            + self.codebook.weight.pow(2).sum(1)\n",
    "        )  # [BHW, num_codes]\n",
    "        \n",
    "        indices = torch.argmin(distances, dim=1)  # 가장 가까운 코드 index 선택\n",
    "        # 선택된 코드북 벡터로 대체 → quantized output\n",
    "        quantized = self.codebook(indices).view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "\n",
    "        commitment_loss = F.mse_loss(quantized.detach(), z) # Loss 1: z와 quantized의 차이 (commitment loss)\n",
    "        codebook_loss = F.mse_loss(quantized, z.detach()) # Loss 2: codebook update (codebook이 z에 가까워지도록)\n",
    "        quantized = z + (quantized - z).detach() # Straight-through estimator: backward는 z 경로로 통과\n",
    "        # total loss, quantized output, index map 반환\n",
    "        return quantized, commitment_loss + codebook_loss, indices.view(B, H, W) \n",
    "\n",
    "\n",
    "# Decoder: quantized feature → 원본 이미지 복원\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dim, hidden_dim, 4, 2, 1),  # 16 → 32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(hidden_dim, hidden_dim, 4, 2, 1),  # 32 → 64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, out_channels, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# ViT Discriminator: Vision Transformer 기반 판별기\n",
    "\n",
    "from torchvision.models.vision_transformer import vit_b_16\n",
    "\n",
    "class ViTDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vit = vit_b_16(pretrained=False) # ViT-Base (pretrained X): patch size 16x16\n",
    "        self.vit.heads = nn.Linear(self.vit.heads.in_features, 1) # 출력층 수정: 이진 분류 (진짜/가짜)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x).squeeze() # [B, 1] → [B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "720ea835-fb29-4841-928c-0c80d6f6a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQGAN Wrapper: encoder + quantizer + decoder\n",
    "class VQGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder() # 이미지 → latent\n",
    "        self.quantizer = VectorQuantizer() # vector quantization\n",
    "        self.decoder = Decoder() # latent → 이미지 복원\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x) # encoder output\n",
    "        z_q, q_loss, _ = self.quantizer(z_e) # vector quantization\n",
    "        x_recon = self.decoder(z_q) # decoder\n",
    "        return x_recon, q_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0d6cc11-d88f-4519-846e-2a035052228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수: 재구성 + VQ + GAN 통합\n",
    "def vqgan_loss(x, x_recon, q_loss, d_real, d_fake):\n",
    "    # Reconstruction loss (L2) (perceptual loss로 대체 가능)\n",
    "    recon_loss = F.mse_loss(x_recon, x)\n",
    "\n",
    "    # Generator loss = 재구성 + VQ loss - GAN reward\n",
    "    g_loss = recon_loss + 1e-6 * q_loss - d_fake.mean()\n",
    "\n",
    "    # Discriminator: 진짜는 1, 가짜는 0 (hinge loss)\n",
    "    # Discriminator hinge loss\n",
    "    d_loss = F.relu(1.0 - d_real).mean() + F.relu(1.0 + d_fake).mean()\n",
    "\n",
    "    return g_loss, d_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

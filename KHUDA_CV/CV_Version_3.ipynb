{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95fa7cb-504b-499b-a7b6-7c74bf8f72c3",
   "metadata": {},
   "source": [
    "# **Code Review_Version3** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1001993-0c60-4b33-90b8-be34343e559b",
   "metadata": {},
   "source": [
    "## **1) Head: ArcFace** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58398c80-8c1f-4744-88b4-ab2f73a7b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "\n",
    "Class ArcFace\n",
    "    1. def __init__(self, in_dim, out_dim, s, m):\n",
    "        - s and m are parameters derived from \"ArcFace: Additive Angular Margin Loss for Deep Face Recognition\".\n",
    "        - Matrix W:\n",
    "            1) The matrix W has dimensions in_dim x out_dim.\n",
    "            2) W is initialized using Xavier initialization.\n",
    "            3) in_dim: Dimensionality of the tensor resulting from flattening the forward pass of VGG19.\n",
    "            4) out_dim: Number of classes.\n",
    "            \n",
    "    2. def forward(self, x):\n",
    "        - the forward pass of the ArcFace model.\n",
    "\n",
    "'''\n",
    "\n",
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, s, m):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.s = s # scaling value\n",
    "        self.m = m # angle margin\n",
    "        self.W = nn.Parameter(torch.empty(in_dim, out_dim)) # weight matrix initialization\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x, W normalization\n",
    "        normalized_x = F.normalize(x, p=2, dim=1)\n",
    "        normalized_W = F.normalize(self.W, p=2, dim=0)\n",
    "\n",
    "        # cosine similarity\n",
    "        cosine = torch.matmul(normalized_x.view(normalized_x.size(0), -1), normalized_W)\n",
    "        \n",
    "        # Using torch.clamp() to ensure cosine values are within a safe range,\n",
    "        # preventing potential NaN losses.\n",
    "        \n",
    "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7)) # angle with input vector and weight vector\n",
    "        \n",
    "        probability = self.s * torch.cos(theta+self.m) \n",
    "        \n",
    "        return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e4616-3496-40f7-971b-9785eef2b90e",
   "metadata": {},
   "source": [
    "#### - **Kaiming 초기화: nn.init.kaiming_uniform_, ReLU 활성화 함수에 적합**\n",
    "#### - **Xavier 초기화: n.init.xavier_uniform_, tanh, simoid 활성화 함수에 적합**\n",
    "#### => **Kaiming 초기화 -> Xavier 초기화 변경 고려 = nn.init.xavier_uniform_(self.W)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8c03e-756a-4eed-a0d5-4c65be0846f8",
   "metadata": {},
   "source": [
    "## **2) Backbone: VGG19 Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495940e-aff4-4116-ae0d-171c787993c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "\n",
    "Class VGG19\n",
    "    1. def __init__(self, base_dim = 64):\n",
    "        1) Hyper parameter\n",
    "        - base_dim is derived from \"VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.\"\n",
    "        - The architecture of this class is partially based on VGG19, with the notable exception that it does not include fully connected layers.\n",
    "    \n",
    "    2. def forward(self, x):\n",
    "        - The forward pass of the VGG19 model.\n",
    "        - EXAMPLE ) input : 224 x 224 x 3 -> output :  7 x 7 x 512\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "def conv_2(in_dim, out_dim):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_dim, out_dim, kernel_size = 3, padding = 1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim, out_dim, kernel_size = 3, padding = 1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def conv_4(in_dim, out_dim):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_dim, out_dim, kernel_size = 3, padding = 1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim, out_dim, kernel_size = 3, padding = 1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim, out_dim, kernel_size = 3, padding = 1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim, out_dim, kernel_size = 3, padding = 1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self, base_dim=64):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "        conv_2(3, base_dim),\n",
    "        conv_2(base_dim, base_dim*2),\n",
    "        conv_4(base_dim*2, base_dim*4),\n",
    "        conv_4(base_dim*4, base_dim*8),\n",
    "        conv_4(base_dim*8, base_dim*8)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): # input: 224 x 224 x 3 -> output: 7 x 7 x 512\n",
    "        x = self.feature(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5178f6-1110-4c24-b31e-c8de81f7061b",
   "metadata": {},
   "source": [
    "## **3) Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a97940-1ceb-4f46-ba5f-9c782f7a96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Backbone import VGG19\n",
    "from ArcFace import ArcFace\n",
    "import torch.nn as nn\n",
    "\n",
    "class Recognizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Recognizer, self).__init__()\n",
    "        self.VGG19 = VGG19()\n",
    "        self.ArcFace = ArcFace(in_dim = 25088, out_dim = 20, s = 64, m = 0.6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.VGG19(x)\n",
    "        x = self.ArcFace(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e63ef-97ba-41fc-8cf1-2f7e57c052cd",
   "metadata": {},
   "source": [
    "#### - **VGG19 Model과 ArcFace 결합**\n",
    "#### - **VGG19를 통과한 input image의 feature vecotr를 ArcFace에 전달하여 classfication based angle**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a48f25-2f35-433e-8e71-6e58dcfeb3b3",
   "metadata": {},
   "source": [
    "## **4) Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a834b1f-bf73-489f-aee9-e781f1c06de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Code written by Hayoung Lee.\n",
    "Contact email: lhayoung9@khu.ac.kr (Note: I may not check my email regularly due to my mistake.)\n",
    "\n",
    "Training conducted using TPU v2 on Google Colaboratory.\n",
    "\n",
    "'''\n",
    "\n",
    "# Dataset Build\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set project folder path\n",
    "\n",
    "project_folder = '/content/drive/MyDrive/Project3'\n",
    "\n",
    "# Initialize lists to store image paths and labels\n",
    "\n",
    "image = []\n",
    "label = []\n",
    "\n",
    "# Traverse through the project folder to collect image paths and corresponding labels\n",
    "\n",
    "for subdir, _, files in os.walk(project_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(('png', 'jpg', 'jpeg')):\n",
    "            image_path = os.path.join(subdir, file)\n",
    "            image.append(image_path)\n",
    "            \n",
    "            label_name = os.path.basename(subdir)\n",
    "            label.append(label_name)\n",
    "            \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from Preprocessing import CustomDataset\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(image, label, test_size = 0.33, random_state = 425)\n",
    "\n",
    "# Create custom datasets and dataloaders\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Declaration of Model, Optimizer, etc.\n",
    "\n",
    "1) Epoch: 100\n",
    "2) Batch size: 128\n",
    "    - Due to the small size of the dataset, batch size was increased based on professor's advice.\n",
    "3) Loss Function: CrossEntropy\n",
    "4) Optimizer: Adam with Learning rate 0.01\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Model import Recognizer\n",
    "\n",
    "EPOCH = 100\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "\n",
    "MODEL = Recognizer().to(DEVICE)\n",
    "LOSS = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = torch.optim.Adam(MODEL.parameters(), lr = 0.001) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Function Definitions\n",
    "\n",
    "1) def compute_accuracy_and_loss(device, model, data_loader):\n",
    "    - Input: device, model, data_loader\n",
    "    - Output: loss / example_num, correct_num / example_num * 100\n",
    "        - 'loss / example_num' represents the average loss.\n",
    "        - 'correct_num / example_num * 100' represents the accuracy percentage.\n",
    "\n",
    "2) def save_weight(model, path):\n",
    "    - This function saves the model's weights at the specified path.\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "def compute_accuracy_and_loss(device, model, data_loader):\n",
    "    loss, example_num, correct_num = 0, 0, 0\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(data_loader):\n",
    "        image = image.to(device)\n",
    "        probability = model(image)\n",
    "        \n",
    "        #Calculate loss using CrossEntropy\n",
    "    \n",
    "        loss += LOSS(probability, label)\n",
    "        \n",
    "        #Calculate accuracy\n",
    "        \n",
    "        _, true_index = torch.max(label, 1)\n",
    "        _, predict_index = torch.max(probability, 1)\n",
    "        \n",
    "        example_num += true_index.size(0)\n",
    "        correct_num += (true_index == predict_index).sum\n",
    "        \n",
    "        print (f'Epoch: {epoch:03d} | '\n",
    "               f'Batch {batch_idx:03d}/{len(data_loader):03d} |'\n",
    "               f'Loss: {loss:03f}')\n",
    "        \n",
    "    return loss/example_num, correct_num/example_num*100\n",
    "\n",
    "\n",
    "def save_weight(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Visualizing model architecture by using tensorboard Library\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "image_for_visualization, label_for_visualization = train_dataset[0]\n",
    "\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(MODEL, image_for_visualization.unsqueeze(0))\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Training\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    MODEL.train()\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        probability = MODEL(image)\n",
    "        \n",
    "        loss = LOSS(probability, label)\n",
    "        \n",
    "        OPTIMIZER.zero_grad()\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        \n",
    "        print (f'Epoch: {epoch:03d} | '\n",
    "               f'Batch: {batch_idx:03d}/{len(train_loader):03d} |'\n",
    "               f'Loss: {loss:03f}')\n",
    "        \n",
    "    MODEL.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_acc = compute_accuracy_and_loss(DEVICE, MODEL, train_loader)\n",
    "        test_loss, test_acc = compute_accuracy_and_loss(DEVICE, MODEL, test_loader)\n",
    "        \n",
    "        # Add scalars to tensorboard for visualization\n",
    "        \n",
    "        writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('train_acc', train_acc, epoch)\n",
    "        writer.add_scalar('test_loss', test_loss, epoch)\n",
    "        writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "        writer.flush()\n",
    "    \n",
    "    # Save model weights every 10 epochs\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        save_weight(MODEL.VGG19, f\"/content/drive/MyDrive/VGG19_{epoch}.pth\")\n",
    "        save_weight(MODEL.ArcFace, f\"/content/drive/MyDrive/ArcFace_{epoch}.pth\")\n",
    "        \n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')\n",
    "\n",
    "elapsed = (time.time() - start_time)/60\n",
    "print(f'Total Training Time: {elapsed:.2f} min')\n",
    "\n",
    "writer.close()\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0769709-4fb3-4515-9c94-8d034d94bca5",
   "metadata": {},
   "source": [
    "#### - **optimmizer 주석에는 0.01로 나와있으나 실제 코드에는 0.001로 반영**\n",
    "#### - **Adam optimier weight_decay(L2 정규화) 추가 고려 (모델 과적합 방지)**\n",
    "#### - **def compute_accuracy_and_loss() 손실 계산 시, 배치 크기 고려 X**\n",
    "#### >>> loss += LOSS(probability, label).item() * image.size(0) # 배치별 손실값을 배치 크기로 나누기\n",
    "#### >>> loss = loss / example_num # 전체 샘플 수로 나누어 평균 손실 계산"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

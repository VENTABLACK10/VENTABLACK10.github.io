{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d8003-8039-4ae2-a377-e799224b7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget https://raw.githubusercontent.com/rickiepark/Generative_Deep_Learning_2nd_Edition/main/notebooks/utils.py\n",
    "    !mkdir -p notebooks\n",
    "    !mv utils.py notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f615749-f130-4ed4-8d20-68f3187823a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    models,\n",
    "    callbacks,\n",
    "    losses,\n",
    "    utils,\n",
    "    metrics,\n",
    "    optimizers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1ecef-0ebe-4115-b102-1a939bd53675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 100\n",
    "EPOCHS = 100\n",
    "LOAD_MODEL = False\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.999\n",
    "LEARNING_RATE = 0.0002\n",
    "NOISE_PARAM = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b77c1e-e319-4609-960f-47b57559b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 정규화 및 크기 변경\n",
    "def preprocess(img):\n",
    "    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\n",
    "    return img\n",
    "\n",
    "\n",
    "train = train_data.map(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158fbac-05ce-4de0-a793-03f02a7a2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN\n",
    "discriminator_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "x = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(\n",
    "    discriminator_input\n",
    ")\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(\n",
    "    128, kernel_size=4, strides=2, padding=\"same\", use_bias=False # bias는 BatchNormalization과 함께 사용 x\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(\n",
    "    256, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(\n",
    "    512, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Conv2D(\n",
    "    1,\n",
    "    kernel_size=4,\n",
    "    strides=1,\n",
    "    padding=\"valid\",\n",
    "    use_bias=False,\n",
    "    activation=\"sigmoid\",\n",
    ")(x)\n",
    "discriminator_output = layers.Flatten()(x)\n",
    "\n",
    "discriminator = models.Model(discriminator_input, discriminator_output)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a98bd4-b6ca-4412-aeaf-efe2393f0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN generator\n",
    "# input = latent vector (Z_DIM 차원)\n",
    "# Conv2DTranspose layer를 통해 점차적 upsampling 진행\n",
    "# 가짜 이미지 생성\n",
    "generator_input = layers.Input(shape=(Z_DIM,))\n",
    "x = layers.Reshape((1, 1, Z_DIM))(generator_input)\n",
    "x = layers.Conv2DTranspose(\n",
    "    512, kernel_size=4, strides=1, padding=\"valid\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Conv2DTranspose(\n",
    "    256, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Conv2DTranspose(\n",
    "    128, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = layers.Conv2DTranspose(\n",
    "    64, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n",
    ")(x)\n",
    "x = layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "generator_output = layers.Conv2DTranspose(\n",
    "    CHANNELS,\n",
    "    kernel_size=4,\n",
    "    strides=2,\n",
    "    padding=\"same\",\n",
    "    use_bias=False,\n",
    "    activation=\"tanh\",\n",
    ")(x)\n",
    "generator = models.Model(generator_input, generator_output)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237ca9f-f147-4aeb-b810-545e9162ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(models.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(DCGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):\n",
    "        super(DCGAN, self).compile()\n",
    "        self.loss_fn = losses.BinaryCrossentropy() # 진짜/가짜를 판별하는 이진 분류 손실함수\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        # loss 및 accuracy를 추적\n",
    "        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\n",
    "        self.d_real_acc_metric = metrics.BinaryAccuracy(name=\"d_real_acc\")\n",
    "        self.d_fake_acc_metric = metrics.BinaryAccuracy(name=\"d_fake_acc\")\n",
    "        self.d_acc_metric = metrics.BinaryAccuracy(name=\"d_acc\")\n",
    "        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n",
    "        self.g_acc_metric = metrics.BinaryAccuracy(name=\"g_acc\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.d_loss_metric,\n",
    "            self.d_real_acc_metric,\n",
    "            self.d_fake_acc_metric,\n",
    "            self.d_acc_metric,\n",
    "            self.g_loss_metric,\n",
    "            self.g_acc_metric,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # 잠재 공간에서 랜덤 포인트 샘플링\n",
    "        # 노이즈로 가짜 이미지 생성\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(batch_size, self.latent_dim)\n",
    "        )\n",
    "\n",
    "        # 이미지 생성 및 판별자 통과\n",
    "        # 가짜 이미지로 판별자 훈련하기\n",
    "        # 생성자는 Z 벡터를 가짜 이미지로 변환\n",
    "        # 판별자는 진짜 이미지와 가짜 이미지를 입력 받아 각 확률값 예측\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_images = self.generator(\n",
    "                random_latent_vectors, training=True\n",
    "            )\n",
    "            real_predictions = self.discriminator(real_images, training=True)\n",
    "            fake_predictions = self.discriminator(\n",
    "                generated_images, training=True\n",
    "            )\n",
    "\n",
    "            # 정답 레이블 및 노이즈 추\n",
    "            real_labels = tf.ones_like(real_predictions)\n",
    "            real_noisy_labels = real_labels + NOISE_PARAM * tf.random.uniform(\n",
    "                tf.shape(real_predictions)\n",
    "            )\n",
    "            fake_labels = tf.zeros_like(fake_predictions)\n",
    "            # NOISE_PARAM만큼 노이즈를 더하거나 빼서 Label Smoothing 효과 발생\n",
    "            # 판별자가 쉽게 overfitting하지 않도록 도움\n",
    "            fake_noisy_labels = fake_labels - NOISE_PARAM * tf.random.uniform(\n",
    "                tf.shape(fake_predictions)\n",
    "            )\n",
    "\n",
    "            # loss 계산\n",
    "            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n",
    "            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2.0\n",
    "\n",
    "            # 생성자 loss는 가짜 이미지를 진짜 이미지처럼 보이게 생성\n",
    "            g_loss = self.loss_fn(real_labels, fake_predictions)\n",
    "\n",
    "        # 기울기 계산 및 적용\n",
    "        gradients_of_discriminator = disc_tape.gradient(\n",
    "            d_loss, self.discriminator.trainable_variables\n",
    "        )\n",
    "        gradients_of_generator = gen_tape.gradient(\n",
    "            g_loss, self.generator.trainable_variables\n",
    "        )\n",
    "\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "        )\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gradients_of_generator, generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # metric update\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.d_real_acc_metric.update_state(real_labels, real_predictions)\n",
    "        self.d_fake_acc_metric.update_state(fake_labels, fake_predictions)\n",
    "        self.d_acc_metric.update_state(\n",
    "            [real_labels, fake_labels], [real_predictions, fake_predictions]\n",
    "        )\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.g_acc_metric.update_state(real_labels, fake_predictions)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f125b9-7e01-444b-9c99-b751211ff206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN 생성\n",
    "dcgan = DCGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=Z_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add9c209-f021-47ee-8082-7aef2ef1aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Training\n",
    "dcgan.compile(\n",
    "    d_optimizer=optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n",
    "    ),\n",
    "    g_optimizer=optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a39ae1-dffd-4cfc-98d4-f943a306197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan.fit(train, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02705bb5-acd7-44a9-9b5c-6fd1a22fca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets\n",
    "\n",
    "num_eps=10 # 학습 epoch \n",
    "bsize=32 # batch size\n",
    "lrate=0.001 # learning_rate\n",
    "lat_dimension=64 # Generator input vector size\n",
    "image_sz=64 # 생성 image size\n",
    "chnls=1 # image channel\n",
    "logging_intv=200 # log output interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a1ccd9-b853-4bd1-abc8-e1f143d4a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANGenerator(nn.Module): # 생성기\n",
    "\n",
    "    # Network 구성\n",
    "    def __init__(self):\n",
    "        super(GANGenerator, self).__init__()\n",
    "        # Linear layer 이후 reshape feature map 크기 설정 \n",
    "        # -> 추후 Upsampling을 통해 크기 복원\n",
    "        self.inp_sz = image_sz // 4\n",
    "        self.lin = nn.Linear(lat_dimension, 128 * self.inp_sz ** 2) # latent vector(z) -> 128 x inp_sz x inp_sz\n",
    "        self.bn1 = nn.BatchNorm2d(128) # 1st Batch Normalization\n",
    "        self.up1 = nn.Upsample(scale_factor=2) # 1st Upsampling (해상도 2배 증가)\n",
    "        self.cn1 = nn.Conv2d(128, 128, 3, stride=1, padding=1) # 128 channel 유지 Conv\n",
    "        self.bn2 = nn.BatchNorm2d(128, 0.8) # 2nd Batch Normalization\n",
    "        self.rl1 = nn.LeakyReLU(0.2, inplace=True) # 1st LeakyReLU, 음수 영역도 일부 허용 -> gradient 소실 방지\n",
    "        self.up2 = nn.Upsample(scale_factor=2) # 2nd Upsampling\n",
    "        self.cn2 = nn.Conv2d(128, 64, 3, stride=1, padding=1) # channel 128 -> 64 Conv\n",
    "        self.bn3 = nn.BatchNorm2d(64, 0.8) # 3rd Batch Normalization\n",
    "        self.rl2 = nn.LeakyReLU(0.2, inplace=True) # 2nd LeakyReLU\n",
    "        self.cn3 = nn.Conv2d(64, chnls, 3, stride=1, padding=1) # Last Conv 64 channels -> image channel 1 or 3\n",
    "        self.act = nn.Tanh() # output normalization (-1 ~ 1) \n",
    "\n",
    "    # 순전파 정의\n",
    "    # input: latent vector(z)\n",
    "    def forward(self, x):\n",
    "        x = self.lin(x) # z -> dense layer\n",
    "        # linear output vector -> reshape 4D Tensor (batch, channel, height, width)  \n",
    "        x = x.view(x.shape[0], 128, self.inp_sz, self.inp_sz)\n",
    "        x = self.bn1(x) # batch normalization\n",
    "        x = self.up1(x) # Upsampling (해상도 2배 증가)\n",
    "        x = self.cn1(x) # convolution 진행\n",
    "        x = self.bn2(x) # batch normalization\n",
    "        x = self.rl1(x) # LeakyReLU\n",
    "        x = self.up2(x) # Upsampling\n",
    "        x = self.cn2(x) # convolution 진행\n",
    "        x = self.bn3(x) # batch normalization\n",
    "        x = self.rl2(x) # LeakyReLU\n",
    "        x = self.cn3(x) # convolution 진행 후 최종 채널 수로 변환\n",
    "        out = self.act(x) # Tanh로 output normalization (-1~1)\n",
    "        return out # 생성된 image 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e6daca-357e-4f7d-b217-18ce946405c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDiscriminator(nn.Module): # 판별기\n",
    "    def __init__(self):\n",
    "        super(GANDiscriminator, self).__init__()\n",
    "\n",
    "        # 하나의 Conv Block 구성\n",
    "        # input_channels, output_channels\n",
    "        def disc_module(ip_chnls, op_chnls, bnorm=True):\n",
    "            mod = [nn.Conv2d(ip_chnls, op_chnls, 3, 2, 1), # kernel 3x3, stride=2 downsampling\n",
    "                   nn.LeakyReLU(0.2, inplace=True), # 비선형성 도입\n",
    "                   nn.Dropout2d(0.25)] # Overfitting Dropout\n",
    "            if bnorm:\n",
    "                mod += [nn.BatchNorm2d(op_chnls, 0.8)] # optional\n",
    "            return mod\n",
    "\n",
    "        # disc_module을 이어붙인 convolution stack\n",
    "        # 4개의 block을 순차적으로 적용 -> channel 증가, 해상도 감소\n",
    "        self.disc_model = nn.Sequential(\n",
    "            *disc_module(chnls, 16, bnorm=False),\n",
    "            *disc_module(16, 32),\n",
    "            *disc_module(32, 64),\n",
    "            *disc_module(64, 128),\n",
    "        )\n",
    "\n",
    "        # width and height down-sized image\n",
    "        ds_size = image_sz // 2 ** 4\n",
    "        self.adverse_lyr = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1),\n",
    "                                         nn.Sigmoid()) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.disc_model(x) # conv block 토과\n",
    "        x = x.view(x.shape[0], -1) # flatten\n",
    "        out = self.adverse_lyr(x) # linear + sigmoid\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69dc09c3-3659-4661-8ac9-71f78c49d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = GANGenerator() # Generator model: random latent vector -> image 생성\n",
    "disc = GANDiscriminator() # Discriminator model: image 진짜/가짜 판별\n",
    "\n",
    "# define the loss metric -> Binary Cross Entropy Loss\n",
    "adv_loss_func = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f198015e-7abd-4a3e-9e50-e593ef9357a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset and corresponding dataloader\n",
    "dloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"./data/mnist/\",\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize((image_sz, image_sz)), # image resize 64x64\n",
    "             transforms.ToTensor(), # image tensor 0~1 \n",
    "             transforms.Normalize([0.5], [0.5])] # image normalization -1~1 \n",
    "        ),\n",
    "    ),\n",
    "    batch_size=bsize,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# define the optimization schedule for both G and D -> Adam\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr=lrate)\n",
    "opt_disc = torch.optim.Adam(disc.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f2de30-6207-411f-ac59-e63270a6a3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 0 | batch number 0 | generator loss = 0.6853175163269043 | discriminator loss = 0.6935094594955444\n",
      "epoch number 0 | batch number 200 | generator loss = 0.9600651860237122 | discriminator loss = 0.596145510673523\n",
      "epoch number 0 | batch number 400 | generator loss = 0.9147008657455444 | discriminator loss = 0.5393365621566772\n",
      "epoch number 0 | batch number 600 | generator loss = 1.2645645141601562 | discriminator loss = 0.4504905343055725\n",
      "epoch number 0 | batch number 800 | generator loss = 1.3696907758712769 | discriminator loss = 0.8016576170921326\n",
      "epoch number 0 | batch number 1000 | generator loss = 0.6775225400924683 | discriminator loss = 0.4123033881187439\n",
      "epoch number 0 | batch number 1200 | generator loss = 3.7660908699035645 | discriminator loss = 0.13774575293064117\n",
      "epoch number 0 | batch number 1400 | generator loss = 3.423884153366089 | discriminator loss = 0.18035462498664856\n",
      "epoch number 0 | batch number 1600 | generator loss = 3.87838077545166 | discriminator loss = 0.417702853679657\n",
      "epoch number 0 | batch number 1800 | generator loss = 4.045217037200928 | discriminator loss = 0.08056224882602692\n",
      "epoch number 1 | batch number 125 | generator loss = 5.8347015380859375 | discriminator loss = 0.19473065435886383\n",
      "epoch number 1 | batch number 325 | generator loss = 2.058969736099243 | discriminator loss = 0.2768264412879944\n",
      "epoch number 1 | batch number 525 | generator loss = 2.8270058631896973 | discriminator loss = 0.3740125894546509\n",
      "epoch number 1 | batch number 725 | generator loss = 3.5544800758361816 | discriminator loss = 0.08384451270103455\n",
      "epoch number 1 | batch number 925 | generator loss = 2.8935251235961914 | discriminator loss = 0.11476018279790878\n",
      "epoch number 1 | batch number 1125 | generator loss = 1.250127911567688 | discriminator loss = 0.6423650979995728\n",
      "epoch number 1 | batch number 1325 | generator loss = 3.5441489219665527 | discriminator loss = 0.16746872663497925\n",
      "epoch number 1 | batch number 1525 | generator loss = 5.097688674926758 | discriminator loss = 0.03284363076090813\n",
      "epoch number 1 | batch number 1725 | generator loss = 4.559228897094727 | discriminator loss = 0.042344190180301666\n",
      "epoch number 2 | batch number 50 | generator loss = 2.0143911838531494 | discriminator loss = 0.19001290202140808\n",
      "epoch number 2 | batch number 250 | generator loss = 3.5581538677215576 | discriminator loss = 0.3016259968280792\n",
      "epoch number 2 | batch number 450 | generator loss = 3.051352024078369 | discriminator loss = 0.0964103564620018\n",
      "epoch number 2 | batch number 650 | generator loss = 5.67398738861084 | discriminator loss = 0.14664840698242188\n",
      "epoch number 2 | batch number 850 | generator loss = 2.4421162605285645 | discriminator loss = 0.1446816325187683\n",
      "epoch number 2 | batch number 1050 | generator loss = 3.1206111907958984 | discriminator loss = 0.19884073734283447\n",
      "epoch number 2 | batch number 1250 | generator loss = 5.391976356506348 | discriminator loss = 0.16870315372943878\n",
      "epoch number 2 | batch number 1450 | generator loss = 5.6850996017456055 | discriminator loss = 0.18594247102737427\n",
      "epoch number 2 | batch number 1650 | generator loss = 4.299696922302246 | discriminator loss = 0.1815948486328125\n",
      "epoch number 2 | batch number 1850 | generator loss = 1.737874150276184 | discriminator loss = 0.2650994658470154\n",
      "epoch number 3 | batch number 175 | generator loss = 6.254986763000488 | discriminator loss = 0.6802733540534973\n",
      "epoch number 3 | batch number 375 | generator loss = 5.265749931335449 | discriminator loss = 0.4358433485031128\n",
      "epoch number 3 | batch number 575 | generator loss = 4.161545276641846 | discriminator loss = 0.053731225430965424\n",
      "epoch number 3 | batch number 775 | generator loss = 0.9459410309791565 | discriminator loss = 0.0949750542640686\n",
      "epoch number 3 | batch number 975 | generator loss = 0.9373248219490051 | discriminator loss = 0.2795403003692627\n",
      "epoch number 3 | batch number 1175 | generator loss = 3.2493176460266113 | discriminator loss = 0.061098817735910416\n",
      "epoch number 3 | batch number 1375 | generator loss = 1.7648183107376099 | discriminator loss = 0.27384141087532043\n",
      "epoch number 3 | batch number 1575 | generator loss = 1.787671446800232 | discriminator loss = 0.12444811314344406\n",
      "epoch number 3 | batch number 1775 | generator loss = 5.949594497680664 | discriminator loss = 0.05656830221414566\n",
      "epoch number 4 | batch number 100 | generator loss = 3.7088310718536377 | discriminator loss = 0.09075399488210678\n",
      "epoch number 4 | batch number 300 | generator loss = 2.5539207458496094 | discriminator loss = 1.2829729318618774\n",
      "epoch number 4 | batch number 500 | generator loss = 1.6565070152282715 | discriminator loss = 0.07662153244018555\n",
      "epoch number 4 | batch number 700 | generator loss = 4.9102702140808105 | discriminator loss = 0.17578531801700592\n",
      "epoch number 4 | batch number 900 | generator loss = 5.649628162384033 | discriminator loss = 0.083333820104599\n",
      "epoch number 4 | batch number 1100 | generator loss = 4.534152984619141 | discriminator loss = 0.3781682550907135\n",
      "epoch number 4 | batch number 1300 | generator loss = 3.683622360229492 | discriminator loss = 0.09746195375919342\n",
      "epoch number 4 | batch number 1500 | generator loss = 5.503308296203613 | discriminator loss = 0.028385447338223457\n",
      "epoch number 4 | batch number 1700 | generator loss = 2.7445931434631348 | discriminator loss = 0.15421590209007263\n",
      "epoch number 5 | batch number 25 | generator loss = 5.303686618804932 | discriminator loss = 0.9929630756378174\n",
      "epoch number 5 | batch number 225 | generator loss = 6.10244607925415 | discriminator loss = 0.10308657586574554\n",
      "epoch number 5 | batch number 425 | generator loss = 4.73200798034668 | discriminator loss = 0.15216007828712463\n",
      "epoch number 5 | batch number 625 | generator loss = 8.731429100036621 | discriminator loss = 0.11611747741699219\n",
      "epoch number 5 | batch number 825 | generator loss = 5.844035625457764 | discriminator loss = 0.20723748207092285\n",
      "epoch number 5 | batch number 1025 | generator loss = 3.3017263412475586 | discriminator loss = 0.09238231182098389\n",
      "epoch number 5 | batch number 1225 | generator loss = 3.785991668701172 | discriminator loss = 0.8690690398216248\n",
      "epoch number 5 | batch number 1425 | generator loss = 3.019169330596924 | discriminator loss = 0.23338168859481812\n",
      "epoch number 5 | batch number 1625 | generator loss = 5.185596466064453 | discriminator loss = 0.04972275719046593\n",
      "epoch number 5 | batch number 1825 | generator loss = 7.639011383056641 | discriminator loss = 0.046059541404247284\n",
      "epoch number 6 | batch number 150 | generator loss = 3.437175750732422 | discriminator loss = 0.06801065802574158\n",
      "epoch number 6 | batch number 350 | generator loss = 1.9803119897842407 | discriminator loss = 0.15977053344249725\n",
      "epoch number 6 | batch number 550 | generator loss = 3.5944137573242188 | discriminator loss = 0.07086243480443954\n",
      "epoch number 6 | batch number 750 | generator loss = 1.072656273841858 | discriminator loss = 0.2659471035003662\n",
      "epoch number 6 | batch number 950 | generator loss = 2.9174509048461914 | discriminator loss = 0.041687823832035065\n",
      "epoch number 6 | batch number 1150 | generator loss = 4.285224914550781 | discriminator loss = 0.06621696054935455\n",
      "epoch number 6 | batch number 1350 | generator loss = 3.8702034950256348 | discriminator loss = 0.13844746351242065\n",
      "epoch number 6 | batch number 1550 | generator loss = 4.015110969543457 | discriminator loss = 0.18095943331718445\n",
      "epoch number 6 | batch number 1750 | generator loss = 7.247861862182617 | discriminator loss = 0.022571289911866188\n",
      "epoch number 7 | batch number 75 | generator loss = 6.552127838134766 | discriminator loss = 0.22319790720939636\n",
      "epoch number 7 | batch number 275 | generator loss = 6.867395401000977 | discriminator loss = 0.012913655489683151\n",
      "epoch number 7 | batch number 475 | generator loss = 4.087368011474609 | discriminator loss = 0.01980246789753437\n",
      "epoch number 7 | batch number 675 | generator loss = 11.508166313171387 | discriminator loss = 0.8707876205444336\n",
      "epoch number 7 | batch number 875 | generator loss = 3.5255751609802246 | discriminator loss = 0.054334238171577454\n",
      "epoch number 7 | batch number 1075 | generator loss = 7.802746295928955 | discriminator loss = 0.019412830471992493\n",
      "epoch number 7 | batch number 1275 | generator loss = 1.7969812154769897 | discriminator loss = 0.7249870300292969\n",
      "epoch number 7 | batch number 1475 | generator loss = 4.319245338439941 | discriminator loss = 0.25785693526268005\n",
      "epoch number 7 | batch number 1675 | generator loss = 5.265085697174072 | discriminator loss = 0.07656516134738922\n",
      "epoch number 8 | batch number 0 | generator loss = 3.968662738800049 | discriminator loss = 0.13134583830833435\n",
      "epoch number 8 | batch number 200 | generator loss = 3.967337131500244 | discriminator loss = 0.08387374132871628\n",
      "epoch number 8 | batch number 400 | generator loss = 5.532919406890869 | discriminator loss = 0.05270100384950638\n",
      "epoch number 8 | batch number 600 | generator loss = 4.891707420349121 | discriminator loss = 0.10082933306694031\n",
      "epoch number 8 | batch number 800 | generator loss = 2.890615701675415 | discriminator loss = 0.05505620688199997\n",
      "epoch number 8 | batch number 1000 | generator loss = 4.220986366271973 | discriminator loss = 0.16951295733451843\n",
      "epoch number 8 | batch number 1200 | generator loss = 2.0223255157470703 | discriminator loss = 0.011934305541217327\n",
      "epoch number 8 | batch number 1400 | generator loss = 6.727531433105469 | discriminator loss = 0.09378514438867569\n",
      "epoch number 8 | batch number 1600 | generator loss = 4.215365409851074 | discriminator loss = 0.017066698521375656\n",
      "epoch number 8 | batch number 1800 | generator loss = 4.3990559577941895 | discriminator loss = 0.1675950586795807\n",
      "epoch number 9 | batch number 125 | generator loss = 4.19144868850708 | discriminator loss = 0.09063012897968292\n",
      "epoch number 9 | batch number 325 | generator loss = 8.071688652038574 | discriminator loss = 0.19945494830608368\n",
      "epoch number 9 | batch number 525 | generator loss = 5.815125465393066 | discriminator loss = 0.05363355204463005\n",
      "epoch number 9 | batch number 725 | generator loss = 0.41376176476478577 | discriminator loss = 0.16206389665603638\n",
      "epoch number 9 | batch number 925 | generator loss = 9.766631126403809 | discriminator loss = 0.30536961555480957\n",
      "epoch number 9 | batch number 1125 | generator loss = 7.232346057891846 | discriminator loss = 0.6358272433280945\n",
      "epoch number 9 | batch number 1325 | generator loss = 3.7630372047424316 | discriminator loss = 0.023632312193512917\n",
      "epoch number 9 | batch number 1525 | generator loss = 7.042107105255127 | discriminator loss = 0.010163049213588238\n",
      "epoch number 9 | batch number 1725 | generator loss = 6.431893348693848 | discriminator loss = 0.09847307950258255\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"./images_mnist\", exist_ok=True)\n",
    "\n",
    "# epoch 10회 반복\n",
    "for ep in range(num_eps):\n",
    "    # batch 반복\n",
    "    for idx, (images, _) in enumerate(dloader):\n",
    "\n",
    "        # truth/fake Ground Truth Label 생성\n",
    "        good_img = Variable(torch.FloatTensor(images.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        bad_img = Variable(torch.FloatTensor(images.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # get a real image\n",
    "        actual_images = Variable(images.type(torch.FloatTensor))\n",
    "\n",
    "        # train the generator model\n",
    "        opt_gen.zero_grad()\n",
    "\n",
    "        # generate a batch of images based on random noise as input\n",
    "        noise = Variable(torch.FloatTensor(np.random.normal(0, 1, (images.shape[0], lat_dimension))))\n",
    "        gen_images = gen(noise)\n",
    "\n",
    "        # generator model optimization - how well can it fool the discriminator\n",
    "        generator_loss = adv_loss_func(disc(gen_images), good_img)\n",
    "        generator_loss.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # train the discriminator model\n",
    "        opt_disc.zero_grad()\n",
    "\n",
    "        # calculate discriminator loss as average of mistakes(losses) in confusing real images as fake and vice versa\n",
    "        actual_image_loss = adv_loss_func(disc(actual_images), good_img)\n",
    "        fake_image_loss = adv_loss_func(disc(gen_images.detach()), bad_img)\n",
    "        discriminator_loss = (actual_image_loss + fake_image_loss) / 2\n",
    "\n",
    "        # discriminator model optimization\n",
    "        discriminator_loss.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        batches_completed = ep * len(dloader) + idx\n",
    "        if batches_completed % logging_intv == 0:\n",
    "            print(f\"epoch number {ep} | batch number {idx} | generator loss = {generator_loss.item()} | discriminator loss = {discriminator_loss.item()}\")\n",
    "            save_image(gen_images.data[:25], f\"images_mnist/{batches_completed}.png\", nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b93b78-0727-47c9-8fc4-e3d1501548f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net 구조의 Generator 구현\n",
    "# image-to-iamge transform 사용\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, chnls_in=3, chnls_op=3):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        # encoder(Downsampling)\n",
    "        # 8단계 encoding layer\n",
    "        # 채널 수 점점 증가/유지, 해상도 감소\n",
    "        # 4~7th layer dropout 적용 -> overfitting 방지\n",
    "        # DownConvBlock, UpConvBlock class 참고\n",
    "        self.down_conv_layer_1 = DownConvBlock(chnls_in, 64, norm=False)\n",
    "        self.down_conv_layer_2 = DownConvBlock(64, 128)\n",
    "        self.down_conv_layer_3 = DownConvBlock(128, 256)\n",
    "        self.down_conv_layer_4 = DownConvBlock(256, 512, dropout=0.5)\n",
    "        self.down_conv_layer_5 = DownConvBlock(512, 512, dropout=0.5)\n",
    "        self.down_conv_layer_6 = DownConvBlock(512, 512, dropout=0.5)\n",
    "        self.down_conv_layer_7 = DownConvBlock(512, 512, dropout=0.5)\n",
    "        self.down_conv_layer_8 = DownConvBlock(512, 512, norm=False, dropout=0.5)\n",
    "\n",
    "        # decoder(Upsampling)\n",
    "        # 각 decoder는 대응되는 encoder의 출력을 skip connection으로 받아 함께 처리\n",
    "        # input channel: 이전 decoder output 512 + 대응 encoder output 512 => concat\n",
    "        self.up_conv_layer_1 = UpConvBlock(512, 512, dropout=0.5)\n",
    "        self.up_conv_layer_2 = UpConvBlock(1024, 512, dropout=0.5)\n",
    "        self.up_conv_layer_3 = UpConvBlock(1024, 512, dropout=0.5)\n",
    "        self.up_conv_layer_4 = UpConvBlock(1024, 512, dropout=0.5)\n",
    "        self.up_conv_layer_5 = UpConvBlock(1024, 256)\n",
    "        self.up_conv_layer_6 = UpConvBlock(512, 128)\n",
    "        self.up_conv_layer_7 = UpConvBlock(256, 64)\n",
    "\n",
    "        # 마지막 upsampling + zero padding + convolution -> 원하는 크기로 맞추기\n",
    "        self.upsample_layer = nn.Upsample(scale_factor=2)\n",
    "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        self.conv_layer_1 = nn.Conv2d(128, chnls_op, 4, padding=1)\n",
    "        self.activation = nn.Tanh() # output normalization -1~1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # image를 점점 압축하며 특징 추출\n",
    "        # 각 단계 결과는 디코딩 단계에서 skip connection으로 재사용\n",
    "        enc1 = self.down_conv_layer_1(x)\n",
    "        enc2 = self.down_conv_layer_2(enc1)\n",
    "        enc3 = self.down_conv_layer_3(enc2)\n",
    "        enc4 = self.down_conv_layer_4(enc3)\n",
    "        enc5 = self.down_conv_layer_5(enc4)\n",
    "        enc6 = self.down_conv_layer_6(enc5)\n",
    "        enc7 = self.down_conv_layer_7(enc6)\n",
    "        enc8 = self.down_conv_layer_8(enc7)\n",
    "\n",
    "        # 각 upsample block은 대응 encoder 출력을 함께 받아 concate후, 처리\n",
    "        # skip connection으로 해상도 정보 + 로컬 정보 보존\n",
    "        dec1 = self.up_conv_layer_1(enc8, enc7)\n",
    "        dec2 = self.up_conv_layer_2(dec1, enc6)\n",
    "        dec3 = self.up_conv_layer_3(dec2, enc5)\n",
    "        dec4 = self.up_conv_layer_4(dec3, enc4)\n",
    "        dec5 = self.up_conv_layer_5(dec4, enc3)\n",
    "        dec6 = self.up_conv_layer_6(dec5, enc2)\n",
    "        dec7 = self.up_conv_layer_7(dec6, enc1)\n",
    "\n",
    "        # 마지막 upsample + zero padding으로 크기 보정\n",
    "        # Conv2D로 최종 output channel 수로 변환\n",
    "        final = self.upsample_layer(dec7)\n",
    "        final = self.zero_pad(final)\n",
    "        final = self.conv_layer_1(final)\n",
    "        return self.activation(final) # Tanh()으로 이미지 픽셀 값 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7adfcda6-9d94-4c27-9189-c7ab43bf3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net decoding module\n",
    "# upsampling + normalization + activation + optional dropout 수행\n",
    "# input tensor와 대응 encoder output tensor를 concat -> skip connection 구현\n",
    "class UpConvBlock(nn.Module):\n",
    "    # ip_sz: input channels\n",
    "    # op_sz: output channels\n",
    "    def __init__(self, ip_sz, op_sz, dropout=0.0):\n",
    "        super(UpConvBlock, self).__init__()\n",
    "        self.layers = [\n",
    "            nn.ConvTranspose2d(ip_sz, op_sz, 4, 2, 1), # upsampling, input 해상도 2배 증가\n",
    "            nn.InstanceNorm2d(op_sz), # instance normalization, small batch에 유리\n",
    "            nn.ReLU(), # 비선형성 추가\n",
    "        ]\n",
    "        if dropout: # optional dropout\n",
    "            self.layers += [nn.Dropout(dropout)]\n",
    "\n",
    "    # x: 이전 decoder output\n",
    "    # enc_ip: skip connection을 위한 encoder 출력\n",
    "    def forward(self, x, enc_ip):\n",
    "        x = nn.Sequential(*(self.layers))(x) # upsample + normalization + activation\n",
    "        op = torch.cat((x, enc_ip), 1) # skip connection: encoder output과 연결\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da3a85f8-6ba2-4c9d-9dab-035f4f14cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding 단계에서 사용되는 Block\n",
    "# Conv -> normalization -> LeakyReLU -> Dropout\n",
    "class DownConvBlock(nn.Module):\n",
    "    # ip_sz: input channels\n",
    "    # op_sz: output channels\n",
    "    # norm: InstanceNorm 사용 여부\n",
    "    def __init__(self, ip_sz, op_sz, norm=True, dropout=0.0):\n",
    "        super(DownConvBlock, self).__init__()\n",
    "        # downsampling\n",
    "        self.layers = [nn.Conv2d(ip_sz, op_sz, 4, 2, 1)] # kernel=4, stride=2, padding=1\n",
    "        if norm: # optional normalization\n",
    "            self.layers.append(nn.InstanceNorm2d(op_sz))\n",
    "        self.layers += [nn.LeakyReLU(0.2)]\n",
    "        if dropout: # optional dropout\n",
    "            self.layers += [nn.Dropout(dropout)]\n",
    "    def forward(self, x):\n",
    "        op = nn.Sequential(*(self.layers))(x) \n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c919068-4a20-425c-b286-5e3df0f3b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pix2Pix 구조에서 사용하는 Discriminator 정의\n",
    "# image를 patch 단위로 분류하는 PatchGAN 사용\n",
    "# input: 진짜 image, 생성 image (2개의 이미지 concat)\n",
    "# output: 각 patch별로 진짜/가짜 판단 map\n",
    "class Pix2PixDiscriminator(nn.Module): # 진짜 이미지 와 가짜 이미지를 비교하여 둘의 차이를 학습\n",
    "    def __init__(self, chnls_in=3):\n",
    "        super(Pix2PixDiscriminator, self).__init__()\n",
    "\n",
    "        # 내부 블록 정의\n",
    "        def disc_conv_block(chnls_in, chnls_op, norm=1):\n",
    "            layers = [nn.Conv2d(chnls_in, chnls_op, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(chnls_op))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        # 입력 이미지와 변환 이미지 2개를 채널 방향으로 concat\n",
    "        # 채널 수를 늘리며 특징 추출\n",
    "        self.lyr1 = disc_conv_block(chnls_in * 2, 64, norm=0)\n",
    "        self.lyr2 = disc_conv_block(64, 128)\n",
    "        self.lyr3 = disc_conv_block(128, 256)\n",
    "        self.lyr4 = disc_conv_block(256, 512)\n",
    "    \n",
    "    def forward(self, real_image, translated_image):\n",
    "        # 두 이미지를 채널 방향으로 합치기\n",
    "        ip = torch.cat((real_image, translated_image), 1)\n",
    "        # discriminator block 적용 -> downsampling을 통한 특징 추출\n",
    "        # output feature map의 공간 해상도는 점점 작아진다.\n",
    "        op = self.lyr1(ip)\n",
    "        op = self.lyr2(op)\n",
    "        op = self.lyr3(op)\n",
    "        op = self.lyr4(op)\n",
    "        # zero padding + 최종 conv\n",
    "        op = nn.ZeroPad2d((1, 0, 1, 0))(op)\n",
    "        op = nn.Conv2d(512, 1, 4, padding=1)(op)\n",
    "        return op # 각 patch마다 진짜/가짜 판단 결과 map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
